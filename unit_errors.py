==================================== ERRORS ====================================
______ ERROR at setup of TestModelOptimizer.test_optimizer_initialization ______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
___ ERROR at setup of TestModelOptimizer.test_parameter_space_initialization ___
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
___ ERROR at setup of TestOptimizationStrategies.test_bayesian_optimization ____
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
__ ERROR at setup of TestOptimizationStrategies.test_grid_search_optimization __
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
_ ERROR at setup of TestOptimizationStrategies.test_random_search_optimization _
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
_ ERROR at setup of TestOptimizationStrategies.test_empty_parameter_space_handling _
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
_ ERROR at setup of TestOptimizationObjectives.test_accuracy_objective_function _
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
_ ERROR at setup of TestOptimizationObjectives.test_confidence_calibration_objective _
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
____ ERROR at setup of TestOptimizationObjectives.test_composite_objective _____
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
_ ERROR at setup of TestOptimizationConstraints.test_time_constraint_enforcement _
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
_ ERROR at setup of TestOptimizationConstraints.test_performance_constraint_validation _
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
_ ERROR at setup of TestOptimizationHistory.test_optimization_history_tracking _
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
_______ ERROR at setup of TestOptimizationHistory.test_parameter_caching _______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
_ ERROR at setup of TestOptimizationHistory.test_performance_history_tracking __
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
__ ERROR at setup of TestOptimizationStatistics.test_success_rate_calculation __
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
_ ERROR at setup of TestOptimizationStatistics.test_average_improvement_tracking _
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
____ ERROR at setup of TestErrorHandling.test_model_training_error_handling ____
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
__ ERROR at setup of TestErrorHandling.test_objective_function_error_handling __
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
__________ ERROR at setup of TestErrorHandling.test_timeout_handling ___________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
_ ERROR at setup of TestPerformanceOptimization.test_optimization_performance_metrics _
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
_ ERROR at setup of TestPerformanceOptimization.test_concurrent_optimizations __
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
___ ERROR at setup of TestPerformanceOptimization.test_memory_usage_tracking ___
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_optimizer.py:72: in mock_drift_detector
    drift_severity=DriftSeverity.LOW,
E   AttributeError: type object 'DriftSeverity' has no attribute 'LOW'
___________ ERROR at setup of TestSensorEvent.test_get_recent_events ___________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/conftest.py:398: in sample_sensor_events
    base_time = datetime.now(datetime.UTC) - timedelta(hours=1)
E   AttributeError: type object 'datetime.datetime' has no attribute 'UTC'
_ ERROR at setup of TestSensorEvent.test_get_recent_events_with_sensor_filter __
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/conftest.py:398: in sample_sensor_events
    base_time = datetime.now(datetime.UTC) - timedelta(hours=1)
E   AttributeError: type object 'datetime.datetime' has no attribute 'UTC'
______ ERROR at setup of TestLSTMPredictor.test_lstm_training_convergence ______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_base_predictors.py:65: in synthetic_training_data
    is_currently_occupied = np.random.binomial(1, base_probability, n_samples)
numpy/random/mtrand.pyx:3460: in numpy.random.mtrand.RandomState.binomial
    ???
_common.pyx:391: in numpy.random._common.check_array_constraint
    ???
_common.pyx:377: in numpy.random._common._check_array_cons_bounded_0_1
    ???
E   ValueError: p < 0, p > 1 or p contains NaNs
_______ ERROR at setup of TestLSTMPredictor.test_lstm_prediction_format ________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_base_predictors.py:65: in synthetic_training_data
    is_currently_occupied = np.random.binomial(1, base_probability, n_samples)
numpy/random/mtrand.pyx:3460: in numpy.random.mtrand.RandomState.binomial
    ???
_common.pyx:391: in numpy.random._common.check_array_constraint
    ???
_common.pyx:377: in numpy.random._common._check_array_cons_bounded_0_1
    ???
E   ValueError: p < 0, p > 1 or p contains NaNs
_______ ERROR at setup of TestLSTMPredictor.test_lstm_sequence_creation ________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_base_predictors.py:65: in synthetic_training_data
    is_currently_occupied = np.random.binomial(1, base_probability, n_samples)
numpy/random/mtrand.pyx:3460: in numpy.random.mtrand.RandomState.binomial
    ???
_common.pyx:391: in numpy.random._common.check_array_constraint
    ???
_common.pyx:377: in numpy.random._common._check_array_cons_bounded_0_1
    ???
E   ValueError: p < 0, p > 1 or p contains NaNs
______ ERROR at setup of TestLSTMPredictor.test_lstm_prediction_intervals ______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_base_predictors.py:65: in synthetic_training_data
    is_currently_occupied = np.random.binomial(1, base_probability, n_samples)
numpy/random/mtrand.pyx:3460: in numpy.random.mtrand.RandomState.binomial
    ???
_common.pyx:391: in numpy.random._common.check_array_constraint
    ???
_common.pyx:377: in numpy.random._common._check_array_cons_bounded_0_1
    ???
E   ValueError: p < 0, p > 1 or p contains NaNs
_________ ERROR at setup of TestXGBoostPredictor.test_xgboost_training _________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_base_predictors.py:65: in synthetic_training_data
    is_currently_occupied = np.random.binomial(1, base_probability, n_samples)
numpy/random/mtrand.pyx:3460: in numpy.random.mtrand.RandomState.binomial
    ???
_common.pyx:391: in numpy.random._common.check_array_constraint
    ???
_common.pyx:377: in numpy.random._common._check_array_cons_bounded_0_1
    ???
E   ValueError: p < 0, p > 1 or p contains NaNs
____ ERROR at setup of TestXGBoostPredictor.test_xgboost_feature_importance ____
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_base_predictors.py:65: in synthetic_training_data
    is_currently_occupied = np.random.binomial(1, base_probability, n_samples)
numpy/random/mtrand.pyx:3460: in numpy.random.mtrand.RandomState.binomial
    ???
_common.pyx:391: in numpy.random._common.check_array_constraint
    ???
_common.pyx:377: in numpy.random._common._check_array_cons_bounded_0_1
    ???
E   ValueError: p < 0, p > 1 or p contains NaNs
__ ERROR at setup of TestXGBoostPredictor.test_xgboost_prediction_confidence ___
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_base_predictors.py:65: in synthetic_training_data
    is_currently_occupied = np.random.binomial(1, base_probability, n_samples)
numpy/random/mtrand.pyx:3460: in numpy.random.mtrand.RandomState.binomial
    ???
_common.pyx:391: in numpy.random._common.check_array_constraint
    ???
_common.pyx:377: in numpy.random._common._check_array_cons_bounded_0_1
    ???
E   ValueError: p < 0, p > 1 or p contains NaNs
__________ ERROR at setup of TestHMMPredictor.test_hmm_state_modeling __________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_base_predictors.py:65: in synthetic_training_data
    is_currently_occupied = np.random.binomial(1, base_probability, n_samples)
numpy/random/mtrand.pyx:3460: in numpy.random.mtrand.RandomState.binomial
    ???
_common.pyx:391: in numpy.random._common.check_array_constraint
    ???
_common.pyx:377: in numpy.random._common._check_array_cons_bounded_0_1
    ???
E   ValueError: p < 0, p > 1 or p contains NaNs
______ ERROR at setup of TestHMMPredictor.test_hmm_confidence_uncertainty ______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_base_predictors.py:65: in synthetic_training_data
    is_currently_occupied = np.random.binomial(1, base_probability, n_samples)
numpy/random/mtrand.pyx:3460: in numpy.random.mtrand.RandomState.binomial
    ???
_common.pyx:391: in numpy.random._common.check_array_constraint
    ???
_common.pyx:377: in numpy.random._common._check_array_cons_bounded_0_1
    ???
E   ValueError: p < 0, p > 1 or p contains NaNs
_ ERROR at setup of TestGaussianProcessPredictor.test_gp_uncertainty_quantification _
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_base_predictors.py:65: in synthetic_training_data
    is_currently_occupied = np.random.binomial(1, base_probability, n_samples)
numpy/random/mtrand.pyx:3460: in numpy.random.mtrand.RandomState.binomial
    ???
_common.pyx:391: in numpy.random._common.check_array_constraint
    ???
_common.pyx:377: in numpy.random._common._check_array_cons_bounded_0_1
    ???
E   ValueError: p < 0, p > 1 or p contains NaNs
_ ERROR at setup of TestGaussianProcessPredictor.test_gp_prediction_intervals __
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_base_predictors.py:65: in synthetic_training_data
    is_currently_occupied = np.random.binomial(1, base_probability, n_samples)
numpy/random/mtrand.pyx:3460: in numpy.random.mtrand.RandomState.binomial
    ???
_common.pyx:391: in numpy.random._common.check_array_constraint
    ???
_common.pyx:377: in numpy.random._common._check_array_cons_bounded_0_1
    ???
E   ValueError: p < 0, p > 1 or p contains NaNs
__ ERROR at setup of TestGaussianProcessPredictor.test_gp_kernel_optimization __
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_base_predictors.py:65: in synthetic_training_data
    is_currently_occupied = np.random.binomial(1, base_probability, n_samples)
numpy/random/mtrand.pyx:3460: in numpy.random.mtrand.RandomState.binomial
    ???
_common.pyx:391: in numpy.random._common.check_array_constraint
    ???
_common.pyx:377: in numpy.random._common._check_array_cons_bounded_0_1
    ???
E   ValueError: p < 0, p > 1 or p contains NaNs
___ ERROR at setup of TestModelComparison.test_model_prediction_consistency ____
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_base_predictors.py:65: in synthetic_training_data
    is_currently_occupied = np.random.binomial(1, base_probability, n_samples)
numpy/random/mtrand.pyx:3460: in numpy.random.mtrand.RandomState.binomial
    ???
_common.pyx:391: in numpy.random._common.check_array_constraint
    ???
_common.pyx:377: in numpy.random._common._check_array_cons_bounded_0_1
    ???
E   ValueError: p < 0, p > 1 or p contains NaNs
__ ERROR at setup of TestModelComparison.test_training_performance_comparison __
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_base_predictors.py:65: in synthetic_training_data
    is_currently_occupied = np.random.binomial(1, base_probability, n_samples)
numpy/random/mtrand.pyx:3460: in numpy.random.mtrand.RandomState.binomial
    ???
_common.pyx:391: in numpy.random._common.check_array_constraint
    ???
_common.pyx:377: in numpy.random._common._check_array_cons_bounded_0_1
    ???
E   ValueError: p < 0, p > 1 or p contains NaNs
=================================== FAILURES ===================================
_____________ TestConfigurationErrors.test_config_validation_error _____________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:161: in test_config_validation_error
    error = ConfigValidationError(
E   TypeError: ConfigValidationError.__init__() got an unexpected keyword argument 'field'
______________ TestConfigurationErrors.test_config_parsing_error _______________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:186: in test_config_parsing_error
    assert error.severity == ErrorSeverity.HIGH
E   assert <ErrorSeverity.CRITICAL: 'critical'> == <ErrorSeverity.HIGH: 'high'>
E    +  where <ErrorSeverity.CRITICAL: 'critical'> = ConfigParsingError("Failed to parse configuration file 'config.yaml': Invalid YAML syntax").severity
E    +  and   <ErrorSeverity.HIGH: 'high'> = ErrorSeverity.HIGH
_________ TestHomeAssistantErrors.test_home_assistant_connection_error _________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:207: in test_home_assistant_connection_error
    assert error.severity == ErrorSeverity.HIGH
E   AssertionError: assert <ErrorSeverity.CRITICAL: 'critical'> == <ErrorSeverity.HIGH: 'high'>
E    +  where <ErrorSeverity.CRITICAL: 'critical'> = HomeAssistantConnectionError('Failed to connect to Home Assistant at http://ha:8123').severity
E    +  and   <ErrorSeverity.HIGH: 'high'> = ErrorSeverity.HIGH
_______ TestHomeAssistantErrors.test_home_assistant_authentication_error _______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:218: in test_home_assistant_authentication_error
    assert error.severity == ErrorSeverity.HIGH
E   AssertionError: assert <ErrorSeverity.CRITICAL: 'critical'> == <ErrorSeverity.HIGH: 'high'>
E    +  where <ErrorSeverity.CRITICAL: 'critical'> = HomeAssistantAuthenticationError('Authentication failed for Home Assistant at http://ha:8123').severity
E    +  and   <ErrorSeverity.HIGH: 'high'> = ErrorSeverity.HIGH
_______________ TestDatabaseErrors.test_database_integrity_error _______________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:327: in test_database_integrity_error
    error = DatabaseIntegrityError(
E   TypeError: DatabaseIntegrityError.__init__() got an unexpected keyword argument 'values'
__________________ TestModelErrors.test_model_training_error ___________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:351: in test_model_training_error
    error = ModelTrainingError(
E   TypeError: ModelTrainingError.__init__() got an unexpected keyword argument 'training_data_size'
_________________ TestModelErrors.test_model_prediction_error __________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:367: in test_model_prediction_error
    error = ModelPredictionError("xgboost", "bedroom", feature_shape=(10, 5))
E   TypeError: ModelPredictionError.__init__() got an unexpected keyword argument 'feature_shape'
__________________ TestModelErrors.test_model_not_found_error __________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:384: in test_model_not_found_error
    assert error.error_code == "MODEL_NOT_FOUND"
E   AssertionError: assert 'MODEL_NOT_FOUND_ERROR' == 'MODEL_NOT_FOUND'
E     - MODEL_NOT_FOUND
E     + MODEL_NOT_FOUND_ERROR
E     ?                ++++++
____________ TestModelErrors.test_insufficient_training_data_error _____________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:390: in test_insufficient_training_data_error
    error = InsufficientTrainingDataError(
E   TypeError: InsufficientTrainingDataError.__init__() got an unexpected keyword argument 'data_points'
______________ TestModelErrors.test_model_version_mismatch_error _______________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:414: in test_model_version_mismatch_error
    assert error.severity == ErrorSeverity.HIGH
E   AssertionError: assert <ErrorSeverity.MEDIUM: 'medium'> == <ErrorSeverity.HIGH: 'high'>
E    +  where <ErrorSeverity.MEDIUM: 'medium'> = ModelVersionMismatchError('Model version mismatch for ensemble in room bathroom: expected 1.0, got 2.0').severity
E    +  and   <ErrorSeverity.HIGH: 'high'> = ErrorSeverity.HIGH
__________ TestFeatureEngineeringErrors.test_feature_extraction_error __________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:428: in test_feature_extraction_error
    error = FeatureExtractionError(
E   TypeError: FeatureExtractionError.__init__() got an unexpected keyword argument 'time_range'
__________ TestFeatureEngineeringErrors.test_feature_validation_error __________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:446: in test_feature_validation_error
    error = FeatureValidationError(
E   TypeError: FeatureValidationError.__init__() got multiple values for argument 'room_id'
___________ TestFeatureEngineeringErrors.test_missing_feature_error ____________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:463: in test_missing_feature_error
    error = MissingFeatureError(
E   TypeError: MissingFeatureError.__init__() got an unexpected keyword argument 'available_features'
____________ TestFeatureEngineeringErrors.test_feature_store_error _____________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:487: in test_feature_store_error
    assert error.context["feature_group"] == "temporal_features"
E   KeyError: 'feature_group'
___________ TestMQTTAndIntegrationErrors.test_mqtt_connection_error ____________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:502: in test_mqtt_connection_error
    error = MQTTConnectionError(
E   TypeError: MQTTConnectionError.__init__() got an unexpected keyword argument 'username'
_____________ TestMQTTAndIntegrationErrors.test_mqtt_publish_error _____________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:518: in test_mqtt_publish_error
    error = MQTTPublishError(
E   TypeError: MQTTPublishError.__init__() got an unexpected keyword argument 'qos'
__________ TestMQTTAndIntegrationErrors.test_mqtt_subscription_error ___________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:532: in test_mqtt_subscription_error
    error = MQTTSubscriptionError("occupancy/+/status")
E   TypeError: MQTTSubscriptionError.__init__() missing 1 required positional argument: 'broker'
___________ TestMQTTAndIntegrationErrors.test_data_validation_error ____________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:552: in test_data_validation_error
    error = DataValidationError(
E   TypeError: DataValidationError.__init__() got an unexpected keyword argument 'sample_data'
_________ TestMQTTAndIntegrationErrors.test_rate_limit_exceeded_error __________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:565: in test_rate_limit_exceeded_error
    error = RateLimitExceededError(
E   TypeError: RateLimitExceededError.__init__() got an unexpected keyword argument 'reset_time'
___________________ TestSystemErrors.test_system_error_base ____________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:584: in test_system_error_base
    error = SystemError("System error")
E   TypeError: SystemError.__init__() missing 1 required positional argument: 'operation'
_______________ TestSystemErrors.test_resource_exhaustion_error ________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:594: in test_resource_exhaustion_error
    assert isinstance(error, SystemError)
E   AssertionError: assert False
E    +  where False = isinstance(ResourceExhaustionError('System resource unavailable: memory - 1024.5MB/1000.0MB'), SystemError)
_______________ TestSystemErrors.test_service_unavailable_error ________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:607: in test_service_unavailable_error
    error = ServiceUnavailableError(
E   TypeError: ServiceUnavailableError.__init__() got an unexpected keyword argument 'endpoint'
_________________ TestSystemErrors.test_maintenance_mode_error _________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:624: in test_maintenance_mode_error
    error = MaintenanceModeError(end_time=end_time)
E   TypeError: MaintenanceModeError.__init__() got an unexpected keyword argument 'end_time'
___________ TestSystemErrors.test_maintenance_mode_error_no_end_time ___________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:634: in test_maintenance_mode_error_no_end_time
    error = MaintenanceModeError()
E   TypeError: MaintenanceModeError.__init__() missing 1 required positional argument: 'component'
______________ TestExceptionIntegration.test_exception_hierarchy _______________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:673: in test_exception_hierarchy
    assert issubclass(DataValidationError, IntegrationError)
E   assert False
E    +  where False = issubclass(DataValidationError, IntegrationError)
____________ TestExceptionIntegration.test_severity_based_handling _____________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:733: in test_severity_based_handling
    medium_error = DataValidationError("API", ["Missing field"])
E   TypeError: DataValidationError.__init__() missing 1 required positional argument: 'actual_value'
_____________ TestExceptionIntegration.test_error_code_uniqueness ______________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:769: in test_error_code_uniqueness
    DataValidationError("source", ["error"]),
E   TypeError: DataValidationError.__init__() missing 1 required positional argument: 'actual_value'
_____________ TestExceptionIntegration.test_context_serialization ______________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_core/test_exceptions.py:786: in test_context_serialization
    error = ModelTrainingError("lstm", "living_room", training_data_size=1000)
E   TypeError: ModelTrainingError.__init__() got an unexpected keyword argument 'training_data_size'
___________________ TestRoomState.test_get_occupancy_history ___________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_data/test_models.py:308: in test_get_occupancy_history
    history = await RoomState.get_occupancy_history(
src/data/storage/models.py:619: in get_occupancy_history
    >= datetime.now(datetime.UTC) - timedelta(hours=hours),
E   AttributeError: type object 'datetime.datetime' has no attribute 'UTC'
_________________ TestPrediction.test_get_pending_validations __________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_data/test_models.py:424: in test_get_pending_validations
    pending = await Prediction.get_pending_validations(
src/data/storage/models.py:927: in get_pending_validations
    cutoff_time = datetime.now(datetime.UTC) - timedelta(hours=cutoff_hours)
E   AttributeError: type object 'datetime.datetime' has no attribute 'UTC'
___________________ TestPrediction.test_get_accuracy_metrics ___________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_data/test_models.py:489: in test_get_accuracy_metrics
    metrics = await Prediction.get_accuracy_metrics(
src/data/storage/models.py:952: in get_accuracy_metrics
    start_time = datetime.now(datetime.UTC) - timedelta(days=days)
E   AttributeError: type object 'datetime.datetime' has no attribute 'UTC'
__________________ TestFeatureStore.test_get_latest_features ___________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_data/test_models.py:654: in test_get_latest_features
    latest = await FeatureStore.get_latest_features(
src/data/storage/models.py:1350: in get_latest_features
    cutoff_time = datetime.now(datetime.UTC) - timedelta(hours=max_age_hours)
E   AttributeError: type object 'datetime.datetime' has no attribute 'UTC'
______________ TestModelIntegration.test_feature_store_lifecycle _______________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_data/test_models.py:1056: in test_feature_store_lifecycle
    latest = await FeatureStore.get_latest_features(
src/data/storage/models.py:1350: in get_latest_features
    cutoff_time = datetime.now(datetime.UTC) - timedelta(hours=max_age_hours)
E   AttributeError: type object 'datetime.datetime' has no attribute 'UTC'
______ TestContextualFeatureExtractor.test_extract_features_comprehensive ______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:128: in test_extract_features_comprehensive
    assert feature in features
E   AssertionError: assert 'temperature_change_rate' in {'avg_door_open_duration': 480.0, 'avg_humidity': 50.0, 'avg_light': 350.0, 'avg_temperature': 23.0, ...}
____ TestContextualFeatureExtractor.test_environmental_features_temperature ____
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:184: in test_environmental_features_temperature
    features = extractor._extract_environmental_features(events)
E   TypeError: ContextualFeatureExtractor._extract_environmental_features() missing 1 required positional argument: 'target_time'
_____ TestContextualFeatureExtractor.test_environmental_features_humidity ______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:215: in test_environmental_features_humidity
    features = extractor._extract_environmental_features(events)
E   TypeError: ContextualFeatureExtractor._extract_environmental_features() missing 1 required positional argument: 'target_time'
_______ TestContextualFeatureExtractor.test_environmental_features_light _______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:241: in test_environmental_features_light
    features = extractor._extract_environmental_features(events)
E   TypeError: ContextualFeatureExtractor._extract_environmental_features() missing 1 required positional argument: 'target_time'
___________ TestContextualFeatureExtractor.test_door_state_features ____________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:276: in test_door_state_features
    features = extractor._extract_door_state_features(events)
E   TypeError: ContextualFeatureExtractor._extract_door_state_features() missing 1 required positional argument: 'target_time'
_____ TestContextualFeatureExtractor.test_multi_room_correlation_features ______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:316: in test_multi_room_correlation_features
    features = extractor._extract_multi_room_features(room_states)
E   TypeError: ContextualFeatureExtractor._extract_multi_room_features() missing 2 required positional arguments: 'room_states' and 'target_time'
____________ TestContextualFeatureExtractor.test_seasonal_features _____________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:340: in test_seasonal_features
    assert "season_indicator" in features
E   AssertionError: assert 'season_indicator' in {'is_autumn': 0.0, 'is_holiday_season': 1.0, 'is_spring': 0.0, 'is_summer': 0.0, ...}
_______ TestContextualFeatureExtractor.test_weather_integration_features _______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:369: in test_weather_integration_features
    features = extractor._extract_environmental_features(events)
E   TypeError: ContextualFeatureExtractor._extract_environmental_features() missing 1 required positional argument: 'target_time'
_________ TestContextualFeatureExtractor.test_cross_sensor_correlation _________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:415: in test_cross_sensor_correlation
    assert features["avg_light_level"] > 200  # Higher than base light level
E   KeyError: 'avg_light_level'
________ TestContextualFeatureExtractor.test_occupancy_spread_analysis _________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:474: in test_occupancy_spread_analysis
    features_concentrated = extractor._extract_multi_room_features(room_states)
E   TypeError: ContextualFeatureExtractor._extract_multi_room_features() missing 2 required positional arguments: 'room_states' and 'target_time'
______ TestContextualFeatureExtractor.test_feature_calculation_edge_cases ______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:569: in test_feature_calculation_edge_cases
    assert features["temperature_stability"] == 1.0  # Perfect stability
E   KeyError: 'temperature_stability'
____ TestContextualFeatureExtractorIntegration.test_realistic_home_scenario ____
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:726: in test_realistic_home_scenario
    assert features["active_rooms_count"] >= 1  # At least kitchen occupied
E   KeyError: 'active_rooms_count'
__ TestContextualFeatureExtractorIntegration.test_seasonal_behavior_patterns ___
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:785: in test_seasonal_behavior_patterns
    assert abs(features["avg_light_level"] - light_level) < 10.0
E   KeyError: 'avg_light_level'
____ TestContextualFeatureExtractorIntegration.test_multi_home_correlation _____
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:834: in test_multi_home_correlation
    features = extractor._extract_multi_room_features(room_states)
E   TypeError: ContextualFeatureExtractor._extract_multi_room_features() missing 2 required positional arguments: 'room_states' and 'target_time'
_______ TestContextualFeatureExtractorEdgeCases.test_mixed_sensor_types ________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:897: in test_mixed_sensor_types
    assert features["avg_light_level"] > 200  # Should average light sensors
E   KeyError: 'avg_light_level'
______ TestContextualFeatureExtractorEdgeCases.test_invalid_sensor_values ______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:929: in test_invalid_sensor_values
    assert features["avg_temperature"] == 22.5
E   assert -488.25 == 22.5
_ TestContextualFeatureExtractorEdgeCases.test_extreme_environmental_conditions _
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:964: in test_extreme_environmental_conditions
    assert features["temperature_stability"] >= 0.0
E   KeyError: 'temperature_stability'
_______ TestContextualFeatureExtractorEdgeCases.test_rapid_state_changes _______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:987: in test_rapid_state_changes
    assert features["door_transition_count"] == 19.0  # 19 transitions
E   assert 20 == 19.0
____ TestContextualFeatureExtractorEdgeCases.test_single_room_multi_states _____
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_contextual.py:1034: in test_single_room_multi_states
    features = extractor._extract_multi_room_features(room_states)
E   TypeError: ContextualFeatureExtractor._extract_multi_room_features() missing 2 required positional arguments: 'room_states' and 'target_time'
_____ TestFeatureEngineeringEngine.test_extractor_partial_failure_handling _____
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
src/features/engineering.py:155: in extract_features
    features = await self._extract_features_parallel(
src/features/engineering.py:339: in _extract_features_parallel
    raise FeatureExtractionError(
E   TypeError: FeatureExtractionError.__init__() got an unexpected keyword argument 'details'
During handling of the above exception, another exception occurred:
tests/unit/test_features/test_engineering.py:665: in test_extractor_partial_failure_handling
    features = await engine.extract_features(
src/features/engineering.py:200: in extract_features
    raise FeatureExtractionError(
E   src.core.exceptions.FeatureExtractionError: Feature extraction failed: general for room living_room | Error Code: FEATURE_EXTRACTION_ERROR | Context: feature_type=general, room_id=living_room | Caused by: TypeError: FeatureExtractionError.__init__() got an unexpected keyword argument 'details'
------------------------------ Captured log call -------------------------------
ERROR    src.features.engineering:engineering.py:335 Failed to extract sequential features: Sequential failed
ERROR    src.features.engineering:engineering.py:199 Feature extraction failed for room living_room: FeatureExtractionError.__init__() got an unexpected keyword argument 'details'
_______ TestFeatureEngineeringEngine.test_initialization_without_config ________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_engineering.py:691: in test_initialization_without_config
    assert engine.config == mock_config
E   AssertionError: assert None == <Mock name='get_config()' spec='SystemConfig' id='139757342382208'>
E    +  where None = <src.features.engineering.FeatureEngineeringEngine object at 0x7f1bcabf6210>.config
____ TestSequentialFeatureExtractorMovementPatterns.test_cat_like_patterns _____
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_sequential.py:705: in test_cat_like_patterns
    assert features["room_revisit_ratio"] > 0.5  # Frequent returns to same room
E   assert 0.4 > 0.5
____________________ TestFeatureRecord.test_is_valid_fresh _____________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_store.py:77: in test_is_valid_fresh
    assert sample_record.is_valid(max_age_hours=24) is True
E   AssertionError: assert False is True
E    +  where False = <bound method FeatureRecord.is_valid of FeatureRecord(room_id='living_room', target_time=datetime.datetime(2024, 1, 15, 15, 0), features={'feature_1': 1.0, 'feature_2': 2.0}, extraction_time=datetime.datetime(2024, 1, 15, 15, 1), lookback_hours=24, feature_types=['temporal', 'sequential'], data_hash='abc123')>(max_age_hours=24)
E    +    where <bound method FeatureRecord.is_valid of FeatureRecord(room_id='living_room', target_time=datetime.datetime(2024, 1, 15, 15, 0), features={'feature_1': 1.0, 'feature_2': 2.0}, extraction_time=datetime.datetime(2024, 1, 15, 15, 1), lookback_hours=24, feature_types=['temporal', 'sequential'], data_hash='abc123')> = FeatureRecord(room_id='living_room', target_time=datetime.datetime(2024, 1, 15, 15, 0), features={'feature_1': 1.0, 'feature_2': 2.0}, extraction_time=datetime.datetime(2024, 1, 15, 15, 1), lookback_hours=24, feature_types=['temporal', 'sequential'], data_hash='abc123').is_valid
________________ TestFeatureRecord.test_is_valid_custom_max_age ________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_store.py:98: in test_is_valid_custom_max_age
    assert sample_record.is_valid(max_age_hours=3) is True
E   AssertionError: assert False is True
E    +  where False = <bound method FeatureRecord.is_valid of FeatureRecord(room_id='living_room', target_time=datetime.datetime(2024, 1, 15, 15, 0), features={'feature_1': 1.0, 'feature_2': 2.0}, extraction_time=datetime.datetime(2024, 1, 15, 15, 1), lookback_hours=24, feature_types=['temporal', 'sequential'], data_hash='abc123')>(max_age_hours=3)
E    +    where <bound method FeatureRecord.is_valid of FeatureRecord(room_id='living_room', target_time=datetime.datetime(2024, 1, 15, 15, 0), features={'feature_1': 1.0, 'feature_2': 2.0}, extraction_time=datetime.datetime(2024, 1, 15, 15, 1), lookback_hours=24, feature_types=['temporal', 'sequential'], data_hash='abc123')> = FeatureRecord(room_id='living_room', target_time=datetime.datetime(2024, 1, 15, 15, 0), features={'feature_1': 1.0, 'feature_2': 2.0}, extraction_time=datetime.datetime(2024, 1, 15, 15, 1), lookback_hours=24, feature_types=['temporal', 'sequential'], data_hash='abc123').is_valid
________________ TestFeatureStore.test_get_features_from_cache _________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_store.py:441: in test_get_features_from_cache
    assert features == expected_features
E   AssertionError: assert {'test_feature': 1.0} == {'cached_feature': 1.0}
E     Left contains 1 more item:
E     {'test_feature': 1.0}
E     Right contains 1 more item:
E     {'cached_feature': 1.0}
E     Full diff:
E     - {'cached_feature': 1.0}
E     ?   ^^^^ ^
E     + {'test_feature': 1.0}
E     ?   ^ ^^
___________ TestFeatureStore.test_get_batch_features_with_exceptions ___________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_store.py:504: in test_get_batch_features_with_exceptions
    assert isinstance(results[1], dict)  # Should be default features
E   AssertionError: assert False
E    +  where False = isinstance(<Mock name='mock._get_default_features()' id='139757343765232'>, dict)
------------------------------ Captured log call -------------------------------
ERROR    src.features.store:store.py:402 Failed to get features for kitchen at 2024-01-15 15:30:00: Feature extraction failed
__________________ TestFeatureStore.test_feature_store_stats ___________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_store.py:594: in test_feature_store_stats
    stats = store.get_statistics()
E   AttributeError: 'FeatureStore' object has no attribute 'get_statistics'
__________ TestTemporalFeatureExtractor.test_event_sequence_patterns ___________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_temporal.py:257: in test_event_sequence_patterns
    assert "transition_regularity" in features
E   AssertionError: assert 'transition_regularity' in {'activity_variance': 0.25, 'avg_off_duration': 1800.0, 'avg_on_duration': 600.0, 'avg_transition_interval': 600.0, ...}
_________ TestTemporalFeatureExtractor.test_cyclical_encoding_accuracy _________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_features/test_temporal.py:289: in test_cyclical_encoding_accuracy
    assert abs(features["hour_sin"] - expected_hour_sin) < 0.0001
E   assert 0.8660254037844385 < 0.0001
E    +  where 0.8660254037844385 = abs((0.8660254037844386 - 1.2246467991473532e-16))
_____________ TestBasePredictor.test_prediction_history_management _____________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_base_predictors.py:270: in test_prediction_history_management
    assert len(predictor.prediction_history) == 500
E   AssertionError: assert 599 == 500
E    +  where 599 = len([(datetime.datetime(2025, 8, 21, 16, 44, 4, 344778), PredictionResult(predicted_time=datetime.datetime(2025, 8, 21, 17, 14, 4, 344778), transition_type='vacant_to_occupied', confidence_score=0.8, prediction_interval=None, alternatives=None, model_type='gp', model_version=None, features_used=None, prediction_metadata=None)), (datetime.datetime(2025, 8, 21, 16, 44, 4, 344778), PredictionResult(predicted_time=datetime.datetime(2025, 8, 21, 17, 14, 4, 344778), transition_type='vacant_to_occupied', confidence_score=0.8, prediction_interval=None, alternatives=None, model_type='gp', model_version=None, features_used=None, prediction_metadata=None)), (datetime.datetime(2025, 8, 21, 16, 44, 4, 344778), PredictionResult(predicted_time=datetime.datetime(2025, 8, 21, 17, 14, 4, 344778), transition_type='vacant_to_occupied', confidence_score=0.8, prediction_interval=None, alternatives=None, model_type='gp', model_version=None, features_used=None, prediction_metadata=None)), (datetime.datetime(2025,
E    +    where [(datetime.datetime(2025, 8, 21, 16, 44, 4, 344778), PredictionResult(predicted_time=datetime.datetime(2025, 8, 21, 17, 14, 4, 344778), transition_type='vacant_to_occupied', confidence_score=0.8, prediction_interval=None, alternatives=None, model_type='gp', model_version=None, features_used=None, prediction_metadata=None)), (datetime.datetime(2025, 8, 21, 16, 44, 4, 344778), PredictionResult(predicted_time=datetime.datetime(2025, 8, 21, 17, 14, 4, 344778), transition_type='vacant_to_occupied', confidence_score=0.8, prediction_interval=None, alternatives=None, model_type='gp', model_version=None, features_used=None, prediction_metadata=None)), (datetime.datetime(2025, 8, 21, 16, 44, 4, 344778), PredictionResult(predicted_time=datetime.datetime(2025, 8, 21, 17, 14, 4, 344778), transition_type='vacant_to_occupied', confidence_score=0.8, prediction_interval=None, alternatives=None, model_type='gp', model_version=None, features_used=None, prediction_metadata=None)), (datetime.datetime(2025, 8, 21, 
__________________ TestLSTMPredictor.test_lstm_initialization __________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_base_predictors.py:366: in test_lstm_initialization
    assert "hidden_size" in predictor.model_params
E   AssertionError: assert 'hidden_size' in {'alpha': 0.0001, 'dropout': 0.2, 'dropout_rate': 0.2, 'early_stopping': False, ...}
E    +  where {'alpha': 0.0001, 'dropout': 0.2, 'dropout_rate': 0.2, 'early_stopping': False, ...} = LSTMPredictor(model_type=lstm, room_id=bedroom, is_trained=False, version=v1.0).model_params
___________________ TestHMMPredictor.test_hmm_initialization ___________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_base_predictors.py:546: in test_hmm_initialization
    assert "n_states" in predictor.model_params
E   AssertionError: assert 'n_states' in {'covariance_type': 'full', 'init_params': 'kmeans', 'max_iter': 100, 'n_components': 4, ...}
E    +  where {'covariance_type': 'full', 'init_params': 'kmeans', 'max_iter': 100, 'n_components': 4, ...} = HMMPredictor(model_type=hmm, room_id=bathroom, is_trained=False, version=v1.0).model_params
_______ TestBasicModelSerialization.test_save_load_trained_xgboost_model _______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
src/models/base/xgboost_predictor.py:283: in predict
    X_scaled = self.feature_scaler.transform(features)
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316: in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:1072: in transform
    check_is_fitted(self)
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/sklearn/utils/validation.py:1754: in check_is_fitted
    raise NotFittedError(msg % {"name": type(estimator).__name__})
E   sklearn.exceptions.NotFittedError: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.
During handling of the above exception, another exception occurred:
tests/unit/test_models/test_model_serialization.py:193: in test_save_load_trained_xgboost_model
    new_predictions = loop.run_until_complete(get_new_predictions())
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/asyncio/base_events.py:691: in run_until_complete
    return future.result()
tests/unit/test_models/test_model_serialization.py:189: in get_new_predictions
    return await new_model.predict(
src/models/base/xgboost_predictor.py:343: in predict
    raise ModelPredictionError(
E   src.core.exceptions.ModelPredictionError: Model prediction failed: xgboost for room test_room | Error Code: MODEL_PREDICTION_ERROR | Context: model_type=xgboost, room_id=test_room | Caused by: NotFittedError: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.
------------------------------ Captured log call -------------------------------
ERROR    src.models.base.xgboost_predictor:xgboost_predictor.py:342 XGBoost prediction failed: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.
____ TestEnsembleModelSerialization.test_ensemble_base_model_serialization _____
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_model_serialization.py:453: in test_ensemble_base_model_serialization
    assert new_ensemble.base_models[model_name].room_id == ensemble.room_id
E   AssertionError: assert 'placeholder' == 'test_room'
E     - test_room
E     + placeholder
________ TestSerializationErrorHandling.test_partial_model_data_loading ________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_model_serialization.py:549: in test_partial_model_data_loading
    pickle.dump(partial_model_data, f)
E   _pickle.PicklingError: Can't pickle <class 'unittest.mock.MagicMock'>: it's not the same object as unittest.mock.MagicMock
___ TestMultipleModelSerialization.test_model_comparison_after_serialization ___
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
src/models/base/xgboost_predictor.py:283: in predict
    X_scaled = self.feature_scaler.transform(features)
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316: in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:1072: in transform
    check_is_fitted(self)
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/sklearn/utils/validation.py:1754: in check_is_fitted
    raise NotFittedError(msg % {"name": type(estimator).__name__})
E   sklearn.exceptions.NotFittedError: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.
During handling of the above exception, another exception occurred:
tests/unit/test_models/test_model_serialization.py:744: in test_model_comparison_after_serialization
    loaded_predictions = loop.run_until_complete(get_loaded_predictions())
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/asyncio/base_events.py:691: in run_until_complete
    return future.result()
tests/unit/test_models/test_model_serialization.py:740: in get_loaded_predictions
    return await loaded_model.predict(
src/models/base/xgboost_predictor.py:343: in predict
    raise ModelPredictionError(
E   src.core.exceptions.ModelPredictionError: Model prediction failed: xgboost for room test_room | Error Code: MODEL_PREDICTION_ERROR | Context: model_type=xgboost, room_id=test_room | Caused by: NotFittedError: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.
------------------------------ Captured log call -------------------------------
ERROR    src.models.base.xgboost_predictor:xgboost_predictor.py:342 XGBoost prediction failed: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.
________ TestBackwardsCompatibility.test_version_compatibility_handling ________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_model_serialization.py:906: in test_version_compatibility_handling
    pickle.dump(old_model_data, f)
E   _pickle.PicklingError: Can't pickle <class 'unittest.mock.MagicMock'>: it's not the same object as unittest.mock.MagicMock
______________ TestTrainingConfigManager.test_profile_management _______________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_training_config.py:456: in test_profile_management
    config_manager.set_current_profile(TrainingProfile("invalid_profile"))
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/enum.py:751: in __call__
    return cls.__new__(cls, value)
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/enum.py:1170: in __new__
    raise ve_exc
E   ValueError: 'invalid_profile' is not a valid TrainingProfile
During handling of the above exception, another exception occurred:
tests/unit/test_models/test_training_config.py:452: in test_profile_management
    with pytest.raises(
E   AssertionError: Regex pattern did not match.
E    Regex: 'Training profile invalid_profile not available'
E    Input: "'invalid_profile' is not a valid TrainingProfile"
________________ TestTrainingConfigManager.test_profile_updates ________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_training_config.py:581: in test_profile_updates
    TrainingProfile("invalid_profile"),
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/enum.py:751: in __call__
    return cls.__new__(cls, value)
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/enum.py:1170: in __new__
    raise ve_exc
E   ValueError: 'invalid_profile' is not a valid TrainingProfile
During handling of the above exception, another exception occurred:
tests/unit/test_models/test_training_config.py:579: in test_profile_updates
    with pytest.raises(ValueError, match="Profile invalid_profile not found"):
E   AssertionError: Regex pattern did not match.
E    Regex: 'Profile invalid_profile not found'
E    Input: "'invalid_profile' is not a valid TrainingProfile"
___________ TestTrainingProgressTracking.test_stage_timing_tracking ____________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_training_pipeline.py:304: in test_stage_timing_tracking
    assert progress.current_stage_start > initial_time
E   TypeError: can't compare offset-naive and offset-aware datetimes
_______ TestDataQualityValidation.test_data_quality_validation_good_data _______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_training_pipeline.py:321: in test_data_quality_validation_good_data
    assert quality_report.passed is True
E   assert True is True
E    +  where True = DataQualityReport(passed=True, total_samples=721, valid_samples=721, sufficient_samples=True, data_freshness_ok=True, feature_completeness_ok=True, temporal_consistency_ok=True, missing_values_percent=0.0, duplicates_count=0, outliers_count=0, data_gaps=[], recommendations=[]).passed
___ TestDataQualityValidation.test_data_quality_validation_insufficient_data ___
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_training_pipeline.py:346: in test_data_quality_validation_insufficient_data
    assert "Insufficient samples" in str(quality_report.recommendations)
E   assert 'Insufficient samples' in "['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.']"
E    +  where "['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.']" = str(['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.'])
E    +    where ['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.'] = DataQualityReport(passed=False, total_samples=0, valid_samples=0, sufficient_samples=False, data_freshness_ok=False, feature_completeness_ok=False, temporal_consistency_ok=False, missing_values_percent=100.0, duplicates_count=0, outliers_count=0, data_gaps=[], recommendations=['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.']).recommendations
------------------------------ Captured log call -------------------------------
ERROR    src.models.training_pipeline:training_pipeline.py:836 Data quality validation failed for room test_room: Cannot subtract tz-naive and tz-aware datetime-like objects.
____ TestDataQualityValidation.test_data_quality_validation_missing_columns ____
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_training_pipeline.py:365: in test_data_quality_validation_missing_columns
    assert "Missing required columns" in str(quality_report.recommendations)
E   assert 'Missing required columns' in "['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.']"
E    +  where "['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.']" = str(['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.'])
E    +    where ['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.'] = DataQualityReport(passed=False, total_samples=0, valid_samples=0, sufficient_samples=False, data_freshness_ok=False, feature_completeness_ok=False, temporal_consistency_ok=False, missing_values_percent=100.0, duplicates_count=0, outliers_count=0, data_gaps=[], recommendations=['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.']).recommendations
------------------------------ Captured log call -------------------------------
ERROR    src.models.training_pipeline:training_pipeline.py:836 Data quality validation failed for room test_room: Cannot subtract tz-naive and tz-aware datetime-like objects.
____ TestDataQualityValidation.test_data_quality_validation_temporal_issues ____
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_training_pipeline.py:388: in test_data_quality_validation_temporal_issues
    assert "chronological order" in str(quality_report.recommendations)
E   assert 'chronological order' in "['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.']"
E    +  where "['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.']" = str(['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.'])
E    +    where ['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.'] = DataQualityReport(passed=False, total_samples=0, valid_samples=0, sufficient_samples=False, data_freshness_ok=False, feature_completeness_ok=False, temporal_consistency_ok=False, missing_values_percent=100.0, duplicates_count=0, outliers_count=0, data_gaps=[], recommendations=['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.']).recommendations
------------------------------ Captured log call -------------------------------
ERROR    src.models.training_pipeline:training_pipeline.py:836 Data quality validation failed for room test_room: Cannot subtract tz-naive and tz-aware datetime-like objects.
__ TestDataQualityValidation.test_data_quality_validation_with_missing_values __
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_training_pipeline.py:407: in test_data_quality_validation_with_missing_values
    assert "High missing values" in str(quality_report.recommendations)
E   assert 'High missing values' in "['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.']"
E    +  where "['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.']" = str(['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.'])
E    +    where ['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.'] = DataQualityReport(passed=False, total_samples=0, valid_samples=0, sufficient_samples=False, data_freshness_ok=False, feature_completeness_ok=False, temporal_consistency_ok=False, missing_values_percent=100.0, duplicates_count=0, outliers_count=0, data_gaps=[], recommendations=['Data quality validation failed: Cannot subtract tz-naive and tz-aware datetime-like objects.']).recommendations
------------------------------ Captured log call -------------------------------
ERROR    src.models.training_pipeline:training_pipeline.py:836 Data quality validation failed for room test_room: Cannot subtract tz-naive and tz-aware datetime-like objects.
____________ TestDataPreparationAndFeatures.test_feature_extraction ____________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_training_pipeline.py:495: in test_feature_extraction
    assert len(features_df) > 0
E   assert 0 > 0
E    +  where 0 = len(Empty DataFrame\nColumns: []\nIndex: [])
------------------------------ Captured log call -------------------------------
ERROR    src.models.training_pipeline:training_pipeline.py:918 Feature extraction failed for room test_room: unsupported operand type(s) for %: 'range' and 'int'
______________ TestDataPreparationAndFeatures.test_data_splitting ______________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_training_pipeline.py:554: in test_data_splitting
    assert len(train_features) == expected_train_size
E   assert 240 == 210
E    +  where 240 = len(     temporal_hour  temporal_day_of_week  ...  contextual_temp  motion_count\n0               16                     0  ...        29.461543             1\n1                5                     5  ...        23.040160             1\n2               12                     1  ...         9.705222             3\n3                8                     1  ...        22.468294             2\n4               23                     2  ...        17.130579             4\n..             ...                   ...  ...              ...           ...\n235             22                     3  ...        19.363807             3\n236             23                     6  ...        23.282016             1\n237             14                     5  ...        18.638156             4\n238              8                     0  ...        24.747470             7\n239             13                     2  ...        19.204178             0\n\n[240 rows x 5 columns])
____________ TestModelTraining.test_model_training_failure_handling ____________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
src/models/training_pipeline.py:1247: in _train_models
    raise ModelTrainingError("ensemble", room_id, cause=None)
E   src.core.exceptions.ModelTrainingError: Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room
During handling of the above exception, another exception occurred:
tests/unit/test_models/test_training_pipeline.py:645: in test_model_training_failure_handling
    await training_pipeline._train_models(
src/models/training_pipeline.py:1256: in _train_models
    raise ModelTrainingError("ensemble", room_id, cause=e)
E   src.core.exceptions.ModelTrainingError: Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: ModelTrainingError: Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room
During handling of the above exception, another exception occurred:
tests/unit/test_models/test_training_pipeline.py:642: in test_model_training_failure_handling
    with pytest.raises(
E   AssertionError: Regex pattern did not match.
E    Regex: 'No models were successfully trained'
E    Input: 'Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: ModelTrainingError: Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room'
------------------------------ Captured log call -------------------------------
ERROR    src.models.training_pipeline:training_pipeline.py:1241 Failed to train ensemble for room test_room: Training failed
ERROR    src.models.training_pipeline:training_pipeline.py:1255 Model training failed for room test_room: Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room
_____________ TestModelTraining.test_model_training_specific_type ______________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
src/models/training_pipeline.py:1247: in _train_models
    raise ModelTrainingError("ensemble", room_id, cause=None)
E   src.core.exceptions.ModelTrainingError: Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room
During handling of the above exception, another exception occurred:
tests/unit/test_models/test_training_pipeline.py:672: in test_model_training_specific_type
    trained_models = await training_pipeline._train_models(
src/models/training_pipeline.py:1256: in _train_models
    raise ModelTrainingError("ensemble", room_id, cause=e)
E   src.core.exceptions.ModelTrainingError: Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: ModelTrainingError: Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room
------------------------------ Captured log call -------------------------------
WARNING  src.models.training_pipeline:training_pipeline.py:1213 Direct training of xgboost not implemented - using ensemble
ERROR    src.models.training_pipeline:training_pipeline.py:1255 Model training failed for room test_room: Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room
__________________ TestModelDeployment.test_model_deployment ___________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_training_pipeline.py:854: in test_model_deployment
    assert len(deployment_info["deployed_models"]) == 1
E   assert 0 == 1
E    +  where 0 = len([])
------------------------------ Captured log call -------------------------------
ERROR    src.models.training_pipeline:training_pipeline.py:1530 Failed to save model artifacts: Can't pickle <class 'unittest.mock.MagicMock'>: it's not the same object as unittest.mock.MagicMock
ERROR    src.models.training_pipeline:training_pipeline.py:1464 Failed to deploy ensemble: Can't pickle <class 'unittest.mock.MagicMock'>: it's not the same object as unittest.mock.MagicMock
________________ TestModelDeployment.test_model_artifact_saving ________________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_training_pipeline.py:891: in test_model_artifact_saving
    artifact_path = await training_pipeline._save_model_artifacts(
src/models/training_pipeline.py:1499: in _save_model_artifacts
    pickle.dump(model, f)
E   _pickle.PicklingError: Can't pickle <class 'unittest.mock.MagicMock'>: it's not the same object as unittest.mock.MagicMock
------------------------------ Captured log call -------------------------------
ERROR    src.models.training_pipeline:training_pipeline.py:1530 Failed to save model artifacts: Can't pickle <class 'unittest.mock.MagicMock'>: it's not the same object as unittest.mock.MagicMock
______ TestFullTrainingWorkflow.test_train_room_models_insufficient_data _______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
src/models/training_pipeline.py:510: in train_room_models
    raise InsufficientTrainingDataError(
E   TypeError: InsufficientTrainingDataError.__init__() got an unexpected keyword argument 'data_points'
During handling of the above exception, another exception occurred:
tests/unit/test_models/test_training_pipeline.py:1001: in test_train_room_models_insufficient_data
    await training_pipeline.train_room_models(
src/models/training_pipeline.py:638: in train_room_models
    raise ModelTrainingError("ensemble", room_id, cause=e)
E   src.core.exceptions.ModelTrainingError: Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: TypeError: InsufficientTrainingDataError.__init__() got an unexpected keyword argument 'data_points'
During handling of the above exception, another exception occurred:
tests/unit/test_models/test_training_pipeline.py:997: in test_train_room_models_insufficient_data
    with pytest.raises(
E   AssertionError: Regex pattern did not match.
E    Regex: 'Model training failed.*Caused by.*Insufficient training data'
E    Input: "Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: TypeError: InsufficientTrainingDataError.__init__() got an unexpected keyword argument 'data_points'"
------------------------------ Captured log call -------------------------------
ERROR    src.models.training_pipeline:training_pipeline.py:637 Training pipeline e9fdfc73-dd5e-416f-bcfe-d5da6dad6e36 failed: InsufficientTrainingDataError.__init__() got an unexpected keyword argument 'data_points'
_______ TestFullTrainingWorkflow.test_train_room_models_quality_failure ________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_training_pipeline.py:1011: in test_train_room_models_quality_failure
    bad_data = pd.DataFrame(
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pandas/core/frame.py:778: in __init__
    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pandas/core/internals/construction.py:503: in dict_to_mgr
    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pandas/core/internals/construction.py:114: in arrays_to_mgr
    index = _extract_index(arrays)
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pandas/core/internals/construction.py:667: in _extract_index
    raise ValueError("If using all scalar values, you must pass an index")
E   ValueError: If using all scalar values, you must pass an index
________ TestFullTrainingWorkflow.test_initial_training_multiple_rooms _________
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_models/test_training_pipeline.py:1039: in test_initial_training_multiple_rooms
    with patch("src.models.training_pipeline.get_config") as mock_get_config:
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/unittest/mock.py:1437: in get_original
    raise AttributeError(
E   AttributeError: <module 'src.models.training_pipeline' from '/home/runner/work/ha-ml-predictor/ha-ml-predictor/src/models/training_pipeline.py'> does not have the attribute 'get_config'
______ TestTrainingPipelineErrorHandling.test_pipeline_exception_handling ______
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
src/models/training_pipeline.py:499: in train_room_models
    raw_data = await self._prepare_training_data(room_id, lookback_days)
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/unittest/mock.py:2291: in _execute_mock_call
    raise effect
E   Exception: Database connection failed
During handling of the above exception, another exception occurred:
tests/unit/test_models/test_training_pipeline.py:1259: in test_pipeline_exception_handling
    await training_pipeline.train_room_models(
src/models/training_pipeline.py:638: in train_room_models
    raise ModelTrainingError("ensemble", room_id, cause=e)
E   src.core.exceptions.ModelTrainingError: Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: Exception: Database connection failed
During handling of the above exception, another exception occurred:
tests/unit/test_models/test_training_pipeline.py:1258: in test_pipeline_exception_handling
    with pytest.raises(ModelTrainingError, match="Training pipeline failed"):
E   AssertionError: Regex pattern did not match.
E    Regex: 'Training pipeline failed'
E    Input: 'Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: Exception: Database connection failed'
------------------------------ Captured log call -------------------------------
ERROR    src.models.training_pipeline:training_pipeline.py:637 Training pipeline 336d3852-e8da-4d3d-8a38-ac84ed3fe504 failed: Database connection failed
__ TestTrainingPipelineErrorHandling.test_retraining_pipeline_error_handling ___
[gw0] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
src/models/training_pipeline.py:438: in run_retraining_pipeline
    return await self.train_room_models(
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/unittest/mock.py:2291: in _execute_mock_call
    raise effect
E   Exception: Retraining failed
During handling of the above exception, another exception occurred:
tests/unit/test_models/test_training_pipeline.py:1329: in test_retraining_pipeline_error_handling
    await training_pipeline.run_retraining_pipeline(
src/models/training_pipeline.py:450: in run_retraining_pipeline
    raise ModelTrainingError("ensemble", room_id, cause=e)
E   src.core.exceptions.ModelTrainingError: Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: Exception: Retraining failed
During handling of the above exception, another exception occurred:
tests/unit/test_models/test_training_pipeline.py:1328: in test_retraining_pipeline_error_handling
    with pytest.raises(ModelTrainingError, match="Retraining pipeline failed"):
E   AssertionError: Regex pattern did not match.
E    Regex: 'Retraining pipeline failed'
E    Input: 'Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: Exception: Retraining failed'
------------------------------ Captured log call -------------------------------
ERROR    src.models.training_pipeline:training_pipeline.py:449 Retraining pipeline failed for test_room: Retraining failed
________ TestPerformanceAndConcurrency.test_background_task_management _________
[gw1] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_tracking_manager.py:873: in test_background_task_management
    assert initial_task_count > 0
E   assert 0 > 0
------------------------------ Captured log setup ------------------------------
ERROR    src.integration.websocket_api:websocket_api.py:681 Failed to start WebSocket API server: [Errno 98] error while attempting to bind on address ('0.0.0.0', 8766): [errno 98] address already in use
ERROR    src.adaptation.tracking_manager:tracking_manager.py:2070 Failed to initialize WebSocket API server: WebSocket connection failed: Connection failed | Error Code: WEBSOCKET_CONNECTION_ERROR | Context: url=Failed to start WebSocket server, reason=Connection failed | Caused by: OSError: [Errno 98] error while attempting to bind on address ('0.0.0.0', 8766): [errno 98] address already in use
________ TestPredictionValidation.test_successful_prediction_validation ________
[gw1] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_validator.py:484: in test_successful_prediction_validation
    assert validation_result.error_minutes == 8.0
E   AttributeError: 'list' object has no attribute 'error_minutes'
___ TestPredictionValidation.test_prediction_validation_multiple_candidates ____
[gw1] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_validator.py:521: in test_prediction_validation_multiple_candidates
    assert validation_result.error_minutes <= 15  # Should be reasonable
E   AttributeError: 'list' object has no attribute 'error_minutes'
_____ TestPredictionValidation.test_validation_with_no_pending_predictions _____
[gw1] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_validator.py:537: in test_validation_with_no_pending_predictions
    assert validation_result is None
E   assert [] is None
___________ TestAccuracyMetricsRetrieval.test_room_accuracy_metrics ____________
[gw1] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_validator.py:768: in test_room_accuracy_metrics
    assert metrics.measurement_period_start is not None
E   assert None is not None
E    +  where None = AccuracyMetrics(total_predictions=0, validated_predictions=0, accurate_predictions=0, expired_predictions=0, failed_predictions=0, accuracy_rate=0.0, mean_error_minutes=0.0, median_error_minutes=0.0, std_error_minutes=0.0, rmse_minutes=0.0, mae_minutes=0.0, avg_error_minutes=None, confidence_calibration=None, correct_predictions=None, error_percentiles={}, accuracy_by_level={}, mean_bias_minutes=0.0, bias_std_minutes=0.0, mean_confidence=0.0, confidence_accuracy_correlation=0.0, overconfidence_rate=0.0, underconfidence_rate=0.0, measurement_period_start=None, measurement_period_end=None, predictions_per_hour=0.0).measurement_period_start
__________ TestAccuracyMetricsRetrieval.test_accuracy_trend_analysis ___________
[gw1] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_validator.py:838: in test_accuracy_trend_analysis
    trend_data = await prediction_validator.get_accuracy_trend(
E   AttributeError: 'PredictionValidator' object has no attribute 'get_accuracy_trend'. Did you mean: 'get_accuracy_metrics'?
_________ TestDatabaseIntegration.test_prediction_storage_to_database __________
[gw1] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/unittest/mock.py:918: in assert_called
    raise AssertionError(msg)
E   AssertionError: Expected 'add' to have been called.
During handling of the above exception, another exception occurred:
tests/unit/test_adaptation/test_validator.py:981: in test_prediction_storage_to_database
    mock_db_session.add.assert_called()
E   AssertionError: Expected 'add' to have been called.
------------------------------ Captured log call -------------------------------
ERROR    src.adaptation.validator:validator.py:1329 Failed to store prediction in database: 'Mock' object does not support the asynchronous context manager protocol
__________ TestDatabaseIntegration.test_validation_update_in_database __________
[gw1] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_validator.py:1004: in test_validation_update_in_database
    await prediction_validator._update_validation_in_db(
E   TypeError: PredictionValidator._update_validation_in_db() got an unexpected keyword argument 'prediction_id'
_______ TestDatabaseIntegration.test_predictions_retrieval_from_database _______
[gw1] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_validator.py:1037: in test_predictions_retrieval_from_database
    assert predictions == mock_predictions
E   AssertionError: assert [] == [<Mock id='14...28063681216'>]
E     Right contains 3 more items, first extra item: <Mock id='140228062626688'>
E     Full diff:
E       [
E     +  ,
E     -  <Mock id='140228062626688'>,
E     -  <Mock id='140228063681600'>,
E     -  <Mock id='140228063681216'>,
E       ]
------------------------------ Captured log call -------------------------------
ERROR    src.adaptation.validator:validator.py:1278 Failed to get predictions from database: 'Mock' object does not support the asynchronous context manager protocol
__________ TestCleanupAndMaintenance.test_validation_history_cleanup ___________
[gw1] linux -- Python 3.12.11 /opt/hostedtoolcache/Python/3.12.11/x64/bin/python
tests/unit/test_adaptation/test_validator.py:1117: in test_validation_history_cleanup
    await prediction_validator.cleanup_old_predictions(days_to_keep=7)
E   TypeError: object int can't be used in 'await' expression
=============================== warnings summary ===============================
../../../../../opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pydantic/_internal/_fields.py:161
../../../../../opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pydantic/_internal/_fields.py:161
  /opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field "model_info" has conflict with protected namespace "model_".
  
  You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.
    warnings.warn(