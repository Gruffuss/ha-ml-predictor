FAILED tests/unit/test_adaptation/test_retrainer.py::TestRetrainingNeedEvaluation::test_cooldown_period_enforcement - AssertionError: assert RetrainingRequest(request_id='bathroom_lstm_1755678029', room_id='bathroom', model_type=<ModelType.LSTM: 'lstm'>, trigger=<RetrainingTrigger.ACCURACY_DEGRADATION: 'accuracy_degradation'>, strategy=<RetrainingStrategy.FULL_RETRAIN: 'full_retrain'>, priority=6.0, created_time=datetime.datetime(2025, 8, 20, 8, 20, 29, 237337), accuracy_metrics=AccuracyMetrics(total_predictions=100, validated_predictions=85, accurate_predictions=45, expired_predictions=0, failed_predictions=0, accuracy_rate=52.9, mean_error_minutes=28.5, median_error_minutes=25.0, std_error_minutes=0.0, rmse_minutes=0.0, mae_minutes=0.0, error_percentiles={}, accuracy_by_level={}, mean_bias_minutes=0.0, bias_std_minutes=0.0, mean_confidence=0.0, confidence_accuracy_correlation=0.68, overconfidence_rate=0.0, underconfidence_rate=0.0, measurement_period_start=datetime.datetime(2025, 8, 19, 8, 20, 29, 235834), measurement_period_end=datetime.datetime(2025, 8, 20, 8, 20, 29, 235859), predictions_per_hour=0.0), drift_metrics=None, performance_degradation={}, retraining_parameters={'lookback_days': 14, 'validation_split': 0.2, 'feature_refresh': True, 'max_training_time_minutes': 60, 'early_stopping_patience': 10, 'min_improvement_threshold': 0.01}, model_hyperparameters={}, feature_engineering_config={}, validation_strategy=['time_series_split', 'holdout'], status=<RetrainingStatus.PENDING: 'pending'>, started_time=None, completed_time=None, error_message=None, execution_log=[], resource_usage_log=[], checkpoint_data={}, training_result=None, performance_improvement={}, prediction_results=[], validation_metrics={}, lookback_days=14, validation_split=0.2, feature_refresh=True) is None
FAILED tests/unit/test_adaptation/test_tracking_manager.py::TestPerformanceAndConcurrency::test_memory_usage_monitoring - assert 100 < 100
FAILED tests/unit/test_adaptation/test_validator.py::TestPredictionRecording::test_prediction_recording_with_metadata - TypeError: 'ValidationRecord' object is not subscriptable
FAILED tests/unit/test_adaptation/test_validator.py::TestPredictionRecording::test_duplicate_prediction_handling - TypeError: object of type 'ValidationRecord' has no len()
FAILED tests/unit/test_adaptation/test_validator.py::TestPredictionRecording::test_prediction_expiration_handling - AttributeError: 'PredictionValidator' object has no attribute '_cleanup_expired_predictions'
FAILED tests/unit/test_adaptation/test_validator.py::TestPredictionValidation::test_successful_prediction_validation - AttributeError: <src.adaptation.validator.PredictionValidator object at 0x7f2472813290> does not have the attribute '_update_validation_in_db'
FAILED tests/unit/test_adaptation/test_validator.py::TestPredictionValidation::test_prediction_validation_multiple_candidates - AttributeError: <src.adaptation.validator.PredictionValidator object at 0x7f24726eaf90> does not have the attribute '_update_validation_in_db'
FAILED tests/unit/test_adaptation/test_validator.py::TestPredictionValidation::test_validation_with_no_pending_predictions - AttributeError: <src.adaptation.validator.PredictionValidator object at 0x7f24726e9e50> does not have the attribute '_update_validation_in_db'
FAILED tests/unit/test_adaptation/test_validator.py::TestPredictionValidation::test_validation_time_window_enforcement - AttributeError: <src.adaptation.validator.PredictionValidator object at 0x7f2472711640> does not have the attribute '_update_validation_in_db'
FAILED tests/unit/test_adaptation/test_validator.py::TestAccuracyMetricsRetrieval::test_time_filtered_accuracy_metrics - TypeError: PredictionValidator.get_accuracy_metrics() got an unexpected keyword argument 'start_time'
FAILED tests/unit/test_adaptation/test_validator.py::TestValidationStatistics::test_validation_stats_collection - TypeError: object dict can't be used in 'await' expression
FAILED tests/unit/test_adaptation/test_validator.py::TestValidationStatistics::test_validation_performance_metrics - AttributeError: 'PredictionValidator' object has no attribute 'get_performance_stats'
FAILED tests/unit/test_adaptation/test_validator.py::TestValidationStatistics::test_total_predictions_counter - AttributeError: 'PredictionValidator' object has no attribute 'get_total_predictions'. Did you mean: '_total_predictions'?
FAILED tests/unit/test_adaptation/test_validator.py::TestValidationStatistics::test_validation_rate_calculation - AttributeError: 'PredictionValidator' object has no attribute 'get_validation_rate'. Did you mean: 'get_validation_stats'?
FAILED tests/unit/test_adaptation/test_validator.py::TestDatabaseIntegration::test_prediction_storage_to_database - TypeError: PredictionValidator._store_prediction_to_db() got an unexpected keyword argument 'room_id'
FAILED tests/unit/test_adaptation/test_validator.py::TestDatabaseIntegration::test_validation_update_in_database - AttributeError: 'PredictionValidator' object has no attribute '_update_validation_in_db'. Did you mean: '_update_predictions_in_db'?
FAILED tests/unit/test_adaptation/test_validator.py::TestDatabaseIntegration::test_predictions_retrieval_from_database - AssertionError: assert [] == [<Mock id='13...94513827584'>]
  Right contains 3 more items, first extra item: <Mock id='139794513827344'>
  Full diff:
    [
  +  ,
  -  <Mock id='139794513827344'>,
  -  <Mock id='139794513827536'>,
  -  <Mock id='139794513827584'>,
    ]
FAILED tests/unit/test_adaptation/test_validator.py::TestCleanupAndMaintenance::test_expired_predictions_cleanup - TypeError: object of type 'ValidationRecord' has no len()
FAILED tests/unit/test_adaptation/test_validator.py::TestCleanupAndMaintenance::test_validation_history_cleanup - AttributeError: 'PredictionValidator' object has no attribute 'cleanup_old_predictions'. Did you mean: 'cleanup_old_records'?
FAILED tests/unit/test_adaptation/test_validator.py::TestCleanupAndMaintenance::test_pending_predictions_size_limit - TypeError: object of type 'ValidationRecord' has no len()
FAILED tests/unit/test_adaptation/test_validator.py::TestErrorHandlingAndEdgeCases::test_validation_with_invalid_actual_time - assert False
 +  where False = isinstance(TypeError("PredictionValidator.validate_prediction() got an unexpected keyword argument 'actual_time'"), (<class 'ValueError'>, <class 'src.core.exceptions.OccupancyPredictionError'>))
FAILED tests/unit/test_adaptation/test_validator.py::TestErrorHandlingAndEdgeCases::test_concurrent_validation_operations - AttributeError: <src.adaptation.validator.PredictionValidator object at 0x7f24725e4bf0> does not have the attribute '_update_validation_in_db'
FAILED tests/unit/test_data/test_database.py::TestDatabaseManager::test_initialize_success - AssertionError: Expected '_create_engine' to have been called once. Called 0 times.
FAILED tests/unit/test_data/test_database.py::TestDatabaseManager::test_verify_connection_success - assert 0 > 0
 +  where 0 = len([])
FAILED tests/unit/test_data/test_database.py::TestDatabaseManager::test_get_session_success - TypeError: '_AsyncGeneratorContextManager' object is not an async iterator
FAILED tests/unit/test_data/test_database.py::TestDatabaseManager::test_get_session_retry_on_connection_error - TypeError: '_AsyncGeneratorContextManager' object is not an async iterator
FAILED tests/unit/test_data/test_database.py::TestDatabaseManager::test_get_session_max_retries_exceeded - TypeError: '_AsyncGeneratorContextManager' object is not an async iterator
FAILED tests/unit/test_data/test_database.py::TestDatabaseManager::test_get_session_non_connection_error - TypeError: '_AsyncGeneratorContextManager' object is not an async iterator
FAILED tests/unit/test_data/test_database.py::TestDatabaseManagerEdgeCases::test_get_session_rollback_on_error - TypeError: '_AsyncGeneratorContextManager' object is not an async iterator
FAILED tests/unit/test_data/test_database.py::TestDatabaseManagerIntegration::test_full_lifecycle - assert False
 +  where False = <src.data.storage.database.DatabaseManager object at 0x7f24726e8470>.is_initialized
FAILED tests/unit/test_data/test_database.py::TestDatabaseManagerIntegration::test_retry_mechanism_with_real_errors - TypeError: '_AsyncGeneratorContextManager' object is not an async iterator
FAILED tests/unit/test_data/test_models.py::TestSensorEvent::test_get_state_changes - LookupError: 'of' is not among the defined enum values. Enum name: sensor_state_enum. Possible values: on, off, open, ..., unknown
FAILED tests/unit/test_data/test_models.py::TestSensorEvent::test_get_transition_sequences - LookupError: 'of' is not among the defined enum values. Enum name: sensor_state_enum. Possible values: on, off, open, ..., unknown
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractor::test_extract_features_comprehensive - AssertionError: assert 'temperature_change_rate' in {'avg_door_open_duration': 480.0, 'avg_humidity': 50.0, 'avg_light': 350.0, 'avg_temperature': 23.0, ...}
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractor::test_environmental_features_temperature - TypeError: ContextualFeatureExtractor._extract_environmental_features() missing 1 required positional argument: 'target_time'
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractor::test_environmental_features_humidity - TypeError: ContextualFeatureExtractor._extract_environmental_features() missing 1 required positional argument: 'target_time'
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractor::test_environmental_features_light - TypeError: ContextualFeatureExtractor._extract_environmental_features() missing 1 required positional argument: 'target_time'
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractor::test_door_state_features - TypeError: ContextualFeatureExtractor._extract_door_state_features() missing 1 required positional argument: 'target_time'
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractor::test_multi_room_correlation_features - TypeError: ContextualFeatureExtractor._extract_multi_room_features() missing 2 required positional arguments: 'room_states' and 'target_time'
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractor::test_seasonal_features - AssertionError: assert 'season_indicator' in {'is_autumn': 0.0, 'is_holiday_season': 1.0, 'is_spring': 0.0, 'is_summer': 0.0, ...}
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractor::test_weather_integration_features - TypeError: ContextualFeatureExtractor._extract_environmental_features() missing 1 required positional argument: 'target_time'
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractor::test_cross_sensor_correlation - KeyError: 'avg_light_level'
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractor::test_occupancy_spread_analysis - TypeError: ContextualFeatureExtractor._extract_multi_room_features() missing 2 required positional arguments: 'room_states' and 'target_time'
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractor::test_environmental_sensor_identification - AttributeError: 'ContextualFeatureExtractor' object has no attribute '_filter_environmental_events'
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractor::test_feature_calculation_edge_cases - KeyError: 'temperature_stability'
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractor::test_default_features_completeness - AssertionError: No features found for category: seasonal
assert False
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractorIntegration::test_realistic_home_scenario - KeyError: 'active_rooms_count'
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractorIntegration::test_seasonal_behavior_patterns - KeyError: 'avg_light_level'
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractorIntegration::test_multi_home_correlation - TypeError: ContextualFeatureExtractor._extract_multi_room_features() missing 2 required positional arguments: 'room_states' and 'target_time'
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractorEdgeCases::test_mixed_sensor_types - KeyError: 'avg_light_level'
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractorEdgeCases::test_invalid_sensor_values - assert -488.25 == 22.5
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractorEdgeCases::test_extreme_environmental_conditions - KeyError: 'temperature_stability'
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractorEdgeCases::test_rapid_state_changes - assert 20 == 19.0
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractorEdgeCases::test_single_room_multi_states - TypeError: ContextualFeatureExtractor._extract_multi_room_features() missing 2 required positional arguments: 'room_states' and 'target_time'
FAILED tests/unit/test_features/test_engineering.py::TestFeatureEngineeringEngine::test_error_handling_extractor_failure - Failed: DID NOT RAISE <class 'src.core.exceptions.FeatureExtractionError'>
FAILED tests/unit/test_features/test_engineering.py::TestFeatureEngineeringEngine::test_validate_configuration_no_config - assert True is False
FAILED tests/unit/test_features/test_sequential.py::TestSequentialFeatureExtractorMovementPatterns::test_cat_like_patterns - assert 0.4 > 0.5
FAILED tests/unit/test_features/test_store.py::TestFeatureCache::test_cache_expired_records - AssertionError: assert {'feature': 1.0} is None
FAILED tests/unit/test_features/test_store.py::TestFeatureStore::test_feature_store_configuration - AttributeError: 'FeatureStore' object has no attribute 'default_lookback_hours'
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractor::test_feature_value_ranges - KeyError: 'day_of_week_sin'
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractor::test_event_sequence_patterns - AssertionError: assert 'transition_regularity' in {'activity_variance': 0.25, 'avg_off_duration': 1800.0, 'avg_on_duration': 600.0, 'avg_transition_interval': 600.0, ...}
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractor::test_cyclical_encoding_accuracy - assert 2.0 < 0.0001
 +  where 2.0 = abs((1.0 - -1.0))
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractor::test_sensor_type_distribution - KeyError: 'motion_sensor_ratio'
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHAEvent::test_ha_event_is_valid_false_missing_entity_id - AssertionError: assert '' is False
 +  where '' = <bound method HAEvent.is_valid of HAEvent(entity_id='', state='on', previous_state='off', timestamp=datetime.datetime(2025, 8, 20, 8, 20, 44, 734873, tzinfo=datetime.timezone.utc), attributes={}, event_type='state_changed')>()
 +    where <bound method HAEvent.is_valid of HAEvent(entity_id='', state='on', previous_state='off', timestamp=datetime.datetime(2025, 8, 20, 8, 20, 44, 734873, tzinfo=datetime.timezone.utc), attributes={}, event_type='state_changed')> = HAEvent(entity_id='', state='on', previous_state='off', timestamp=datetime.datetime(2025, 8, 20, 8, 20, 44, 734873, tzinfo=datetime.timezone.utc), attributes={}, event_type='state_changed').is_valid
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHomeAssistantClient::test_test_authentication_401 - TypeError: 'int' object is not subscriptable
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHomeAssistantClient::test_authenticate_websocket_auth_failed - TypeError: 'int' object is not subscriptable
FAILED tests/unit/test_models/test_ensemble.py::TestEnsembleTraining::test_ensemble_model_weight_calculation - AssertionError: assert 'lstm' == 'xgboost'
  - xgboost
  + lstm
FAILED tests/unit/test_models/test_ensemble.py::TestEnsemblePrediction::test_ensemble_prediction_error_handling - src.core.exceptions.ModelPredictionError: Model prediction failed: ensemble for room test_room | Error Code: MODEL_PREDICTION_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: Exception: Model prediction failed
FAILED tests/unit/test_models/test_ensemble.py::TestEnsembleIncrementalUpdate::test_ensemble_incremental_update - src.core.exceptions.ModelTrainingError: Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: ValueError: Found input variables with inconsistent numbers of samples: [160, 640]
FAILED tests/unit/test_models/test_ensemble.py::TestEnsembleIncrementalUpdate::test_incremental_update_error_handling - src.core.exceptions.ModelTrainingError: Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: ModelTrainingError: Model training failed: ensemble for room test_room | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: ValueError: Input X contains NaN.
LinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values
FAILED tests/unit/test_models/test_ensemble.py::TestEnsemblePerformance::test_ensemble_prediction_latency - AssertionError: assert 1 == 20
 +  where 1 = len([PredictionResult(predicted_time=datetime.datetime(2025, 8, 20, 8, 50, 57, 993630), transition_type='vacant_to_occupied', confidence_score=0.7999999455269103, prediction_interval=None, alternatives=[(datetime.datetime(2025, 8, 20, 8, 50, 57, 992023), 0.8), (datetime.datetime(2025, 8, 20, 8, 50, 57, 992305), 0.8), (datetime.datetime(2025, 8, 20, 8, 50, 57, 992509), 0.8)], model_type='ensemble', model_version='v1.0', features_used=['hour_sin', 'hour_cos', 'weekday', 'prev_state_duration', 'transition_count_1h', 'state_stability', 'temp', 'temp_squared', 'motion_intensity', 'smooth_pattern', 'smooth_trend', 'feature_0', 'feature_1', 'feature_2', 'feature_3'], prediction_metadata={'time_until_transition_seconds': 1800.0, 'prediction_method': 'stacking_ensemble', 'base_model_predictions': {'lstm': 1799.998393, 'xgboost': 1799.998675, 'hmm': 1799.998879, 'gp': 1799.99907}, 'model_weights': {'lstm': 0.25, 'xgboost': 0.35, 'hmm': 0.2, 'gp': 0.2}, 'meta_learner_type': 'xgboost', 'combination_method': 'meta_learner_weighted'})])
 +  and   20 = len(         hour_sin      hour_cos  weekday  ...  feature_1  feature_2  feature_3\n640 -1.000000e+00 -1.836970e-16        0  ...  -0.991091   0.209686   1.295007\n641  2.588190e-01 -9.659258e-01        1  ...   1.329794   0.457542  -0.282125\n642 -7.071068e-01  7.071068e-01        2  ...  -1.675580   0.433748  -0.630864\n643  1.000000e+00  6.123234e-17        0  ...  -0.806520  -1.771897  -0.864430\n644  7.071068e-01 -7.071068e-01        5  ...   1.677201   0.636870   0.692776\n645 -1.000000e+00 -1.836970e-16        0  ...  -0.435139  -0.671015  -0.204707\n646  1.000000e+00  6.123234e-17        1  ...  -1.314879  -1.095839  -0.386495\n647  5.000000e-01  8.660254e-01        4  ...   0.219072  -1.104452  -0.813665\n648  1.224647e-16 -1.000000e+00        0  ...  -0.475392   0.433542  -0.769666\n649  1.224647e-16 -1.000000e+00        6  ...  -0.835870  -0.222575   0.320058\n650 -9.659258e-01 -2.588190e-01        0  ...  -1.095871  -1.681969  -1.315438\n651 -9.659258e-01  2.588190e-01        5  ...   2.768374   0.478038   0.994869\n652  9.659258e-01 -2.588190e-01        5  ...  -0.251552  -1.439608  -0.533845\n653  8.660254e-01 -5.000000e-01        2  ...  -1.925567   0.139821  -0.149636\n654  1.000000e+00  6.123234e-17        5  ...   1.493710   0.238499  -1.336978\n655  0.000000e+00  1.000000e+00        3  ...   0.611071   0.885577  -0.012366\n656  5.000000e-01  8.660254e-01        1  ...  -0.766466   1.781685  -0.328321\n657  1.224647e-16 -1.000000e+00        1  ...  -0.933119  -1.364676  -0.050081\n658 -8.660254e-01 -5.000000e-01        4  ...   0.673473  -0.052474   1.471709\n659 -2.588190e-01  9.659258e-01        4  ...  -0.246918  -0.276335   0.177698\n\n[20 rows x 15 columns])
FAILED tests/unit/test_models/test_model_serialization.py::TestBasicModelSerialization::test_save_load_trained_xgboost_model - src.core.exceptions.ModelPredictionError: Model prediction failed: xgboost for room test_room | Error Code: MODEL_PREDICTION_ERROR | Context: model_type=xgboost, room_id=test_room | Caused by: NotFittedError: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.
FAILED tests/unit/test_models/test_model_serialization.py::TestEnsembleModelSerialization::test_ensemble_base_model_serialization - AssertionError: assert 'placeholder' == 'test_room'
  - test_room
  + placeholder
FAILED tests/unit/test_models/test_model_serialization.py::TestSerializationErrorHandling::test_partial_model_data_loading - _pickle.PicklingError: Can't pickle <class 'unittest.mock.MagicMock'>: it's not the same object as unittest.mock.MagicMock
FAILED tests/unit/test_models/test_model_serialization.py::TestMultipleModelSerialization::test_model_comparison_after_serialization - src.core.exceptions.ModelPredictionError: Model prediction failed: xgboost for room test_room | Error Code: MODEL_PREDICTION_ERROR | Context: model_type=xgboost, room_id=test_room | Caused by: NotFittedError: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.
FAILED tests/unit/test_models/test_model_serialization.py::TestBackwardsCompatibility::test_version_compatibility_handling - _pickle.PicklingError: Can't pickle <class 'unittest.mock.MagicMock'>: it's not the same object as unittest.mock.MagicMock
FAILED tests/unit/test_adaptation/test_tracking_manager.py::TestPerformanceAndConcurrency::test_background_task_management - assert 0 > 0
ERROR tests/unit/test_core/test_exceptions.py
ERROR tests/unit/test_models/test_base_predictors.py
ERROR tests/unit/test_models/test_training_config.py
ERROR tests/unit/test_models/test_training_pipeline.py
ERROR tests/unit/test_adaptation/test_optimizer.py::TestModelOptimizer::test_optimizer_initialization - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestModelOptimizer::test_parameter_space_initialization - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestOptimizationStrategies::test_bayesian_optimization - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestOptimizationStrategies::test_grid_search_optimization - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestOptimizationStrategies::test_random_search_optimization - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestOptimizationStrategies::test_empty_parameter_space_handling - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestOptimizationObjectives::test_accuracy_objective_function - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestOptimizationObjectives::test_confidence_calibration_objective - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestOptimizationObjectives::test_composite_objective - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestOptimizationConstraints::test_time_constraint_enforcement - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestOptimizationConstraints::test_performance_constraint_validation - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestOptimizationHistory::test_optimization_history_tracking - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestOptimizationHistory::test_parameter_caching - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestOptimizationHistory::test_performance_history_tracking - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestOptimizationStatistics::test_success_rate_calculation - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestOptimizationStatistics::test_average_improvement_tracking - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestErrorHandling::test_model_training_error_handling - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestErrorHandling::test_objective_function_error_handling - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestErrorHandling::test_timeout_handling - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestPerformanceOptimization::test_optimization_performance_metrics - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestPerformanceOptimization::test_concurrent_optimizations - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_optimizer.py::TestPerformanceOptimization::test_memory_usage_tracking - TypeError: AccuracyMetrics.__init__() got an unexpected keyword argument 'avg_error_minutes'
ERROR tests/unit/test_adaptation/test_validator.py::TestAccuracyMetricsRetrieval::test_room_accuracy_metrics - TypeError: 'actual_time' is an invalid keyword argument for Prediction
ERROR tests/unit/test_adaptation/test_validator.py::TestAccuracyMetricsRetrieval::test_overall_accuracy_metrics - TypeError: 'actual_time' is an invalid keyword argument for Prediction
ERROR tests/unit/test_adaptation/test_validator.py::TestAccuracyMetricsRetrieval::test_model_specific_accuracy_metrics - TypeError: 'actual_time' is an invalid keyword argument for Prediction
ERROR tests/unit/test_adaptation/test_validator.py::TestAccuracyMetricsRetrieval::test_accuracy_trend_analysis - TypeError: 'actual_time' is an invalid keyword argument for Prediction
ERROR tests/unit/test_adaptation/test_validator.py::TestValidationStatistics::test_room_prediction_counts - TypeError: 'actual_time' is an invalid keyword argument for Prediction
ERROR tests/unit/test_features/test_store.py::TestFeatureStore::test_store_initialization - TypeError: FeatureStore.__init__() got an unexpected keyword argument 'feature_engine'
ERROR tests/unit/test_features/test_store.py::TestFeatureStore::test_get_features_from_cache - TypeError: FeatureStore.__init__() got an unexpected keyword argument 'feature_engine'
ERROR tests/unit/test_features/test_store.py::TestFeatureStore::test_get_features_cache_miss - TypeError: FeatureStore.__init__() got an unexpected keyword argument 'feature_engine'
ERROR tests/unit/test_features/test_store.py::TestFeatureStore::test_get_batch_features_success - TypeError: FeatureStore.__init__() got an unexpected keyword argument 'feature_engine'
ERROR tests/unit/test_features/test_store.py::TestFeatureStore::test_get_batch_features_with_exceptions - TypeError: FeatureStore.__init__() got an unexpected keyword argument 'feature_engine'
ERROR tests/unit/test_features/test_store.py::TestFeatureStore::test_compute_training_data - TypeError: FeatureStore.__init__() got an unexpected keyword argument 'feature_engine'
ERROR tests/unit/test_features/test_store.py::TestFeatureStore::test_get_data_for_features_with_db - TypeError: FeatureStore.__init__() got an unexpected keyword argument 'feature_engine'
ERROR tests/unit/test_features/test_store.py::TestFeatureStore::test_feature_store_stats - TypeError: FeatureStore.__init__() got an unexpected keyword argument 'feature_engine'
ERROR tests/unit/test_features/test_store.py::TestFeatureStore::test_cache_key_generation - TypeError: FeatureStore.__init__() got an unexpected keyword argument 'feature_engine'
========== 77 failed, 462 passed, 26674 warnings, 40 errors in 54.92s ==========