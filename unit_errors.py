=========================== short test summary info ============================
FAILED tests/unit/test_adaptation/test_optimizer.py::TestOptimizationStrategies::test_bayesian_optimization - AssertionError: assert 0 > 0
 +  where 0 = OptimizationResult(success=False, optimization_time_seconds=0.000385, best_parameters={}, best_score=0.0, improvement_over_default=0.0, total_evaluations=0, convergence_achieved=False, optimization_history=[], validation_score=None, training_score=None, cross_validation_scores=None, model_complexity=None, prediction_latency_ms=None, memory_usage_mb=None, error_message='No valid dimensions for optimization').total_evaluations
FAILED tests/unit/test_adaptation/test_optimizer.py::TestOptimizationConstraints::test_performance_constraint_validation - AssertionError: assert False
 +  where False = OptimizationResult(success=False, optimization_time_seconds=0.000205, best_parameters={}, best_score=0.0, improvement_over_default=0.0, total_evaluations=0, convergence_achieved=False, optimization_history=[], validation_score=None, training_score=None, cross_validation_scores=None, model_complexity=None, prediction_latency_ms=None, memory_usage_mb=None, error_message='No valid dimensions for optimization').success
FAILED tests/unit/test_adaptation/test_retrainer.py::TestErrorHandlingAndRecovery::test_model_training_failure_handling - AssertionError: assert <RetrainingStatus.COMPLETED: 'completed'> == <RetrainingStatus.FAILED: 'failed'>
 +  where <RetrainingStatus.COMPLETED: 'completed'> = RetrainingRequest(request_id='failure_test_001', room_id='failure_room', model_type=<ModelType.LSTM: 'lstm'>, trigger=<RetrainingTrigger.MANUAL_REQUEST: 'manual_request'>, strategy=<RetrainingStrategy.FULL_RETRAIN: 'full_retrain'>, priority=5.0, created_time=datetime.datetime(2025, 8, 18, 11, 20, 27, 580793), accuracy_metrics=None, drift_metrics=None, performance_degradation={}, retraining_parameters={'lookback_days': 14, 'validation_split': 0.2, 'feature_refresh': True, 'max_training_time_minutes': 60, 'early_stopping_patience': 10, 'min_improvement_threshold': 0.01}, model_hyperparameters={}, feature_engineering_config={}, validation_strategy=['time_series_split', 'holdout'], status=<RetrainingStatus.COMPLETED: 'completed'>, started_time=datetime.datetime(2025, 8, 18, 11, 20, 27, 580841), completed_time=datetime.datetime(2025, 8, 18, 11, 20, 27, 584821), error_message='Model failure_room_lstm not found in registry | Error Code: RETRAINING_ERROR', execution_log=[], resource_usage_log=[], checkpoint_data={}, training_result=None, performance_improvement={}, prediction_results=[], validation_metrics={}, lookback_days=14, validation_split=0.2, feature_refresh=True).status
 +  and   <RetrainingStatus.FAILED: 'failed'> = RetrainingStatus.FAILED
FAILED tests/unit/test_adaptation/test_retrainer.py::TestErrorHandlingAndRecovery::test_missing_model_handling - assert <RetrainingStatus.COMPLETED: 'completed'> == <RetrainingStatus.FAILED: 'failed'>
 +  where <RetrainingStatus.COMPLETED: 'completed'> = RetrainingRequest(request_id='missing_model_001', room_id='nonexistent_room', model_type='nonexistent_model', trigger=<RetrainingTrigger.MANUAL_REQUEST: 'manual_request'>, strategy=<RetrainingStrategy.FULL_RETRAIN: 'full_retrain'>, priority=5.0, created_time=datetime.datetime(2025, 8, 18, 11, 20, 27, 782595), accuracy_metrics=None, drift_metrics=None, performance_degradation={}, retraining_parameters={'lookback_days': 14, 'validation_split': 0.2, 'feature_refresh': True, 'max_training_time_minutes': 60, 'early_stopping_patience': 10, 'min_improvement_threshold': 0.01}, model_hyperparameters={}, feature_engineering_config={}, validation_strategy=['time_series_split', 'holdout'], status=<RetrainingStatus.COMPLETED: 'completed'>, started_time=datetime.datetime(2025, 8, 18, 11, 20, 27, 782632), completed_time=datetime.datetime(2025, 8, 18, 11, 20, 27, 785627), error_message="'str' object has no attribute 'value'", execution_log=[], resource_usage_log=[], checkpoint_data={}, training_result=None, performance_improvement={}, prediction_results=[], validation_metrics={}, lookback_days=14, validation_split=0.2, feature_refresh=True).status
 +  and   <RetrainingStatus.FAILED: 'failed'> = RetrainingStatus.FAILED
FAILED tests/unit/test_adaptation/test_retrainer.py::TestErrorHandlingAndRecovery::test_insufficient_data_handling - AssertionError: assert <RetrainingStatus.COMPLETED: 'completed'> == <RetrainingStatus.FAILED: 'failed'>
 +  where <RetrainingStatus.COMPLETED: 'completed'> = RetrainingRequest(request_id='insufficient_data_001', room_id='empty_room', model_type=<ModelType.LSTM: 'lstm'>, trigger=<RetrainingTrigger.ACCURACY_DEGRADATION: 'accuracy_degradation'>, strategy=<RetrainingStrategy.FULL_RETRAIN: 'full_retrain'>, priority=7.0, created_time=datetime.datetime(2025, 8, 18, 11, 20, 27, 963923), accuracy_metrics=None, drift_metrics=None, performance_degradation={}, retraining_parameters={'lookback_days': 14, 'validation_split': 0.2, 'feature_refresh': True, 'max_training_time_minutes': 60, 'early_stopping_patience': 10, 'min_improvement_threshold': 0.01}, model_hyperparameters={}, feature_engineering_config={}, validation_strategy=['time_series_split', 'holdout'], status=<RetrainingStatus.COMPLETED: 'completed'>, started_time=datetime.datetime(2025, 8, 18, 11, 20, 27, 963962), completed_time=datetime.datetime(2025, 8, 18, 11, 20, 27, 966651), error_message='Model empty_room_lstm not found in registry | Error Code: RETRAINING_ERROR', execution_log=[], resource_usage_log=[], checkpoint_data={}, training_result=None, performance_improvement={}, prediction_results=[], validation_metrics={}, lookback_days=14, validation_split=0.2, feature_refresh=True).status
 +  and   <RetrainingStatus.FAILED: 'failed'> = RetrainingStatus.FAILED
FAILED tests/unit/test_adaptation/test_retrainer.py::TestErrorHandlingAndRecovery::test_retraining_timeout_handling - assert <RetrainingStatus.COMPLETED: 'completed'> == <RetrainingStatus.FAILED: 'failed'>
 +  where <RetrainingStatus.COMPLETED: 'completed'> = RetrainingRequest(request_id='timeout_test_001', room_id='timeout_room', model_type='slow_model', trigger=<RetrainingTrigger.MANUAL_REQUEST: 'manual_request'>, strategy=<RetrainingStrategy.FULL_RETRAIN: 'full_retrain'>, priority=5.0, created_time=datetime.datetime(2025, 8, 18, 11, 20, 28, 24648), accuracy_metrics=None, drift_metrics=None, performance_degradation={}, retraining_parameters={'lookback_days': 14, 'validation_split': 0.2, 'feature_refresh': True, 'max_training_time_minutes': 60, 'early_stopping_patience': 10, 'min_improvement_threshold': 0.01}, model_hyperparameters={}, feature_engineering_config={}, validation_strategy=['time_series_split', 'holdout'], status=<RetrainingStatus.COMPLETED: 'completed'>, started_time=datetime.datetime(2025, 8, 18, 11, 20, 28, 25007), completed_time=datetime.datetime(2025, 8, 18, 11, 20, 28, 27777), error_message="'str' object has no attribute 'value'", execution_log=[], resource_usage_log=[], checkpoint_data={}, training_result=None, performance_improvement={}, prediction_results=[], validation_metrics={}, lookback_days=14, validation_split=0.2, feature_refresh=True).status
 +  and   <RetrainingStatus.FAILED: 'failed'> = RetrainingStatus.FAILED
FAILED tests/unit/test_adaptation/test_validator.py::TestErrorHandlingAndEdgeCases::test_memory_usage_with_large_datasets - AttributeError: <src.adaptation.validator.PredictionValidator object at 0x7f7a99ae4410> does not have the attribute '_get_predictions_from_db'
FAILED tests/unit/test_data/test_database.py::TestDatabaseManager::test_initialize_success - assert False
 +  where False = <src.data.storage.database.DatabaseManager object at 0x7f7a99a2fc80>.is_initialized
FAILED tests/unit/test_data/test_database.py::TestDatabaseManager::test_verify_connection_success - AttributeError: __aenter__
FAILED tests/unit/test_data/test_database.py::TestDatabaseManager::test_execute_query_success - src.core.exceptions.DatabaseQueryError: Database query failed: SELECT 1... | Error Code: DB_QUERY_ERROR | Context: query=SELECT 1, parameters={'param': 'value'} | Caused by: TypeError: 'coroutine' object does not support the asynchronous context manager protocol
FAILED tests/unit/test_data/test_database.py::TestDatabaseManager::test_health_check_healthy - AssertionError: assert 'unhealthy' == 'healthy'
  - healthy
  + unhealthy
  ? ++
FAILED tests/unit/test_data/test_database.py::TestDatabaseManager::test_health_check_timescaledb_available - AssertionError: assert None == 'available'
FAILED tests/unit/test_data/test_database.py::TestDatabaseManager::test_health_check_timescaledb_unavailable - AssertionError: assert None == 'unavailable'
FAILED tests/unit/test_data/test_database.py::TestDatabaseManager::test_health_check_timescaledb_version_parsing - AttributeError: 'TestDatabaseManager' object has no attribute 'subTest'
FAILED tests/unit/test_data/test_database.py::TestDatabaseManager::test_health_check_timescaledb_version_parsing_error - AssertionError: assert None == 'available'
FAILED tests/unit/test_data/test_database.py::TestDatabaseManager::test_close_cleanup - TypeError: object Mock can't be used in 'await' expression
FAILED tests/unit/test_data/test_database.py::TestGlobalDatabaseFunctions::test_get_db_session - TypeError: 'async_generator' object does not support the asynchronous context manager protocol
FAILED tests/unit/test_data/test_database.py::TestDatabaseManagerEdgeCases::test_verify_connection_timescaledb_warning - AttributeError: __aenter__
FAILED tests/unit/test_data/test_database.py::TestDatabaseManagerEdgeCases::test_get_session_rollback_on_error - AssertionError: Expected 'close' to have been called once. Called 2 times.
Calls: [call(), call()].
FAILED tests/unit/test_data/test_database.py::TestDatabaseManagerEdgeCases::test_health_check_with_previous_errors - AssertionError: assert 'unhealthy' == 'healthy'
  - healthy
  + unhealthy
  ? ++
FAILED tests/unit/test_adaptation/test_tracking_manager.py::TestTrackingManagerInitialization::test_manager_shutdown - assert not True
 +  where True = <src.adaptation.tracking_manager.TrackingManager object at 0x7efd35bea4b0>._tracking_active
FAILED tests/unit/test_data/test_database.py::TestDatabaseManagerIntegration::test_full_lifecycle - src.core.exceptions.DatabaseConnectionError: Failed to connect to database: postgresql+asyncpg://localhost/testdb | Error Code: DB_CONNECTION_ERROR | Context: connection_string=postgresql+asyncpg://localhost/testdb | Caused by: OSError: Multiple exceptions: [Errno 111] Connect call failed ('::1', 5432, 0, 0), [Errno 111] Connect call failed ('127.0.0.1', 5432)
FAILED tests/unit/test_data/test_database.py::TestDatabaseManagerIntegration::test_concurrent_sessions - src.core.exceptions.DatabaseConnectionError: Failed to connect to database: postgresql+asyncpg://localhost/testdb | Error Code: DB_CONNECTION_ERROR | Context: connection_string=postgresql+asyncpg://localhost/testdb | Caused by: OSError: Multiple exceptions: [Errno 111] Connect call failed ('::1', 5432, 0, 0), [Errno 111] Connect call failed ('127.0.0.1', 5432)
FAILED tests/unit/test_adaptation/test_tracking_manager.py::TestPredictionRecording::test_prediction_recording - assert 0 > 0
 +  where 0 = <src.adaptation.tracking_manager.TrackingManager object at 0x7efd35574470>._total_predictions_recorded
FAILED tests/unit/test_data/test_database.py::TestDatabaseManagerIntegration::test_retry_mechanism_with_real_errors - src.core.exceptions.DatabaseConnectionError: Failed to connect to database: postgresql+asyncpg://localhost/testdb | Error Code: DB_CONNECTION_ERROR | Context: connection_string=postgresql+asyncpg://localhost/testdb | Caused by: OSError: Multiple exceptions: [Errno 111] Connect call failed ('::1', 5432, 0, 0), [Errno 111] Connect call failed ('127.0.0.1', 5432)
FAILED tests/unit/test_adaptation/test_tracking_manager.py::TestPredictionRecording::test_prediction_mqtt_integration - AssertionError: Expected 'publish_prediction' to have been called once. Called 0 times.
FAILED tests/unit/test_adaptation/test_tracking_manager.py::TestRoomStateChangeHandling::test_room_state_change_handling - assert 0 > 0
 +  where 0 = <src.adaptation.tracking_manager.TrackingManager object at 0x7efd35c1ecf0>._total_validations_performed
FAILED tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractor::test_natural_light_patterns - assert 1.0 == 0.0
FAILED tests/unit/test_features/test_engineering.py::TestFeatureEngineeringEngine::test_extract_features_parallel - TypeError: FeatureExtractionError.__init__() missing 1 required positional argument: 'room_id'
FAILED tests/unit/test_features/test_engineering.py::TestFeatureEngineeringEngine::test_extract_features_sequential - AssertionError: Expected 'extract_features' to have been called once. Called 0 times.
FAILED tests/unit/test_features/test_engineering.py::TestFeatureEngineeringEngine::test_error_handling_invalid_room_id - TypeError: FeatureExtractionError.__init__() missing 1 required positional argument: 'room_id'
FAILED tests/unit/test_features/test_engineering.py::TestFeatureEngineeringEngine::test_error_handling_extractor_failure - Failed: DID NOT RAISE <class 'src.core.exceptions.FeatureExtractionError'>
FAILED tests/unit/test_features/test_engineering.py::TestFeatureEngineeringEngine::test_validate_configuration_no_config - assert True is False
FAILED tests/unit/test_features/test_engineering.py::TestFeatureEngineeringEngine::test_parallel_vs_sequential_consistency - TypeError: FeatureExtractionError.__init__() missing 1 required positional argument: 'room_id'
FAILED tests/unit/test_features/test_engineering.py::TestFeatureEngineeringEngine::test_extractor_partial_failure_handling - TypeError: FeatureExtractionError.__init__() missing 1 required positional argument: 'room_id'
FAILED tests/unit/test_features/test_engineering.py::TestFeatureEngineeringEngine::test_large_feature_set_handling - TypeError: FeatureExtractionError.__init__() missing 1 required positional argument: 'room_id'
FAILED tests/unit/test_features/test_engineering.py::TestFeatureEngineeringEngine::test_performance_comparison[True] - TypeError: FeatureExtractionError.__init__() missing 1 required positional argument: 'room_id'
FAILED tests/unit/test_features/test_engineering.py::TestFeatureEngineeringEngine::test_performance_comparison[False] - assert 0.014394760131835938 > 0.025
FAILED tests/unit/test_features/test_sequential.py::TestSequentialFeatureExtractor::test_extract_features_multi_room - src.core.exceptions.FeatureExtractionError: Feature extraction failed for sequential features in room 'living_room' | Error Code: FEATURE_EXTRACTION_ERROR | Context: feature_type=sequential, room_id=living_room | Caused by: AttributeError: Mock object has no attribute 'sensors'
FAILED tests/unit/test_features/test_sequential.py::TestSequentialFeatureExtractor::test_extract_features_single_room - src.core.exceptions.FeatureExtractionError: Feature extraction failed for sequential features in room 'living_room' | Error Code: FEATURE_EXTRACTION_ERROR | Context: feature_type=sequential, room_id=living_room | Caused by: AttributeError: Mock object has no attribute 'sensors'
FAILED tests/unit/test_adaptation/test_tracking_manager.py::TestDriftDetectionIntegration::test_manual_drift_detection - assert None is not None
FAILED tests/unit/test_features/test_sequential.py::TestSequentialFeatureExtractor::test_empty_room_configs - AssertionError: assert 'human_movement_probability' in {'active_room_count': 2, 'avg_event_interval': 106.66666666666667, 'avg_room_dwell_time': 320.0, 'burst_ratio': 0.0, ...}
FAILED tests/unit/test_features/test_sequential.py::TestSequentialFeatureExtractor::test_no_classifier_available - KeyError: 'human_movement_probability'
FAILED tests/unit/test_adaptation/test_tracking_manager.py::TestRetrainingIntegration::test_manual_retraining_request - assert None is not None
FAILED tests/unit/test_features/test_sequential.py::TestSequentialFeatureExtractorMovementPatterns::test_cat_like_patterns - assert 0.4 > 0.5
FAILED tests/unit/test_features/test_store.py::TestFeatureCache::test_get_expired_item - AssertionError: assert {'feature_1': 1.0, 'feature_2': 2.0} is None
FAILED tests/unit/test_features/test_store.py::TestFeatureCache::test_get_stats - assert 0 == 1
FAILED tests/unit/test_features/test_store.py::TestFeatureCache::test_feature_type_order_independence - AssertionError: assert 'db705acd2620...645398feca521' == '8212428ee93e...e95b4ba33722f'
  - 8212428ee93eac2863de95b4ba33722f
  + db705acd26200dae5ab645398feca521
FAILED tests/unit/test_features/test_store.py::TestFeatureStore::test_get_data_for_features_with_db - assert 0 == 2
 +  where 0 = len([])
FAILED tests/unit/test_adaptation/test_tracking_manager.py::TestRetrainingIntegration::test_retraining_status_tracking - assert None is not None
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractor::test_extract_features_with_sample_data - src.core.exceptions.FeatureExtractionError: Feature extraction failed for temporal features in room 'living_room' | Error Code: FEATURE_EXTRACTION_ERROR | Context: feature_type=temporal, room_id=living_room | Caused by: TypeError: 'Mock' object is not iterable
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractor::test_extract_features_empty_events - assert 0.0 == -0.7071067811865471
 +  where -0.7071067811865471 = <built-in function sin>((((2 * 3.141592653589793) * 15) / 24))
 +    where <built-in function sin> = math.sin
 +    and   3.141592653589793 = math.pi
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractor::test_extract_features_single_event - src.core.exceptions.FeatureExtractionError: Feature extraction failed for temporal features in room 'living_room' | Error Code: FEATURE_EXTRACTION_ERROR | Context: feature_type=temporal, room_id=living_room | Caused by: TypeError: 'Mock' object is not iterable
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractor::test_feature_consistency - src.core.exceptions.FeatureExtractionError: Feature extraction failed for temporal features in room 'living_room' | Error Code: FEATURE_EXTRACTION_ERROR | Context: feature_type=temporal, room_id=living_room | Caused by: TypeError: 'Mock' object is not iterable
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractor::test_batch_feature_extraction - src.core.exceptions.FeatureExtractionError: Feature extraction failed for temporal features in room 'living_room' | Error Code: FEATURE_EXTRACTION_ERROR | Context: feature_type=temporal, room_id=living_room | Caused by: TypeError: 'Mock' object is not iterable
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractor::test_edge_case_time_boundaries - assert -0.8660254037844384 == 0.0
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractor::test_large_event_sequences - src.core.exceptions.FeatureExtractionError: Feature extraction failed for temporal features in room 'test_room' | Error Code: FEATURE_EXTRACTION_ERROR | Context: feature_type=temporal, room_id=test_room | Caused by: TypeError: 'Mock' object is not iterable
FAILED tests/unit/test_adaptation/test_tracking_manager.py::TestRetrainingIntegration::test_retraining_cancellation - assert False
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractor::test_memory_efficiency - src.core.exceptions.FeatureExtractionError: Feature extraction failed for temporal features in room 'living_room' | Error Code: FEATURE_EXTRACTION_ERROR | Context: feature_type=temporal, room_id=living_room | Caused by: TypeError: 'Mock' object is not iterable
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractor::test_feature_value_ranges - src.core.exceptions.FeatureExtractionError: Feature extraction failed for temporal features in room 'living_room' | Error Code: FEATURE_EXTRACTION_ERROR | Context: feature_type=temporal, room_id=living_room | Caused by: TypeError: 'Mock' object is not iterable
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractor::test_concurrent_extraction - src.core.exceptions.FeatureExtractionError: Feature extraction failed for temporal features in room 'living_room' | Error Code: FEATURE_EXTRACTION_ERROR | Context: feature_type=temporal, room_id=living_room | Caused by: TypeError: 'Mock' object is not iterable
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractorEdgeCases::test_events_in_future - src.core.exceptions.FeatureExtractionError: Feature extraction failed for temporal features in room 'test_room' | Error Code: FEATURE_EXTRACTION_ERROR | Context: feature_type=temporal, room_id=test_room | Caused by: TypeError: 'Mock' object is not iterable
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractorEdgeCases::test_duplicate_timestamps - src.core.exceptions.FeatureExtractionError: Feature extraction failed for temporal features in room 'test_room' | Error Code: FEATURE_EXTRACTION_ERROR | Context: feature_type=temporal, room_id=test_room | Caused by: ZeroDivisionError: float division by zero
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractorEdgeCases::test_extreme_time_differences - src.core.exceptions.FeatureExtractionError: Feature extraction failed for temporal features in room 'test_room' | Error Code: FEATURE_EXTRACTION_ERROR | Context: feature_type=temporal, room_id=test_room | Caused by: TypeError: 'Mock' object is not iterable
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractorEdgeCases::test_rapid_state_changes - src.core.exceptions.FeatureExtractionError: Feature extraction failed for temporal features in room 'test_room' | Error Code: FEATURE_EXTRACTION_ERROR | Context: feature_type=temporal, room_id=test_room | Caused by: TypeError: 'Mock' object is not iterable
FAILED tests/unit/test_features/test_temporal.py::TestTemporalFeatureExtractorEdgeCases::test_missing_sensor_types - src.core.exceptions.FeatureExtractionError: Feature extraction failed for temporal features in room 'test_room' | Error Code: FEATURE_EXTRACTION_ERROR | Context: feature_type=temporal, room_id=test_room | Caused by: TypeError: 'Mock' object is not iterable
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHAEvent::test_ha_event_is_valid_false_missing_entity_id - AssertionError: assert '' is False
 +  where '' = <bound method HAEvent.is_valid of HAEvent(entity_id='', state='on', previous_state='off', timestamp=datetime.datetime(2025, 8, 18, 11, 20, 44, 166715), attributes={}, event_type='state_changed')>()
 +    where <bound method HAEvent.is_valid of HAEvent(entity_id='', state='on', previous_state='off', timestamp=datetime.datetime(2025, 8, 18, 11, 20, 44, 166715), attributes={}, event_type='state_changed')> = HAEvent(entity_id='', state='on', previous_state='off', timestamp=datetime.datetime(2025, 8, 18, 11, 20, 44, 166715), attributes={}, event_type='state_changed').is_valid
FAILED tests/unit/test_ingestion/test_ha_client.py::TestRateLimiter::test_rate_limiter_init - AttributeError: 'RateLimiter' object has no attribute 'window_seconds'
FAILED tests/unit/test_ingestion/test_ha_client.py::TestRateLimiter::test_rate_limiter_acquire_at_limit - src.core.exceptions.RateLimitExceededError: Rate limit exceeded for home_assistant_api: 2 requests per 60s | Error Code: RATE_LIMIT_EXCEEDED_ERROR | Context: service=home_assistant_api, limit=2, window_seconds=60, reset_time=59
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHomeAssistantClient::test_ha_client_init - AttributeError: 'RateLimiter' object has no attribute 'window_seconds'
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHomeAssistantClient::test_test_authentication_success - TypeError: 'coroutine' object does not support the asynchronous context manager protocol
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHomeAssistantClient::test_test_authentication_401 - TypeError: 'coroutine' object does not support the asynchronous context manager protocol
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHomeAssistantClient::test_test_authentication_other_error - TypeError: 'coroutine' object does not support the asynchronous context manager protocol
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHomeAssistantClient::test_test_authentication_connection_error - TypeError: 'coroutine' object does not support the asynchronous context manager protocol
FAILED tests/unit/test_adaptation/test_tracking_manager.py::TestSystemStatusAndMetrics::test_tracking_status_comprehensive - assert 'tracking_active' in {'error': "object Mock can't be used in 'await' expression"}
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHomeAssistantClient::test_connect_websocket_success - TypeError: object AsyncMock can't be used in 'await' expression
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHomeAssistantClient::test_get_entity_state_success - TypeError: 'coroutine' object does not support the asynchronous context manager protocol
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHomeAssistantClient::test_get_entity_state_not_found - TypeError: 'coroutine' object does not support the asynchronous context manager protocol
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHomeAssistantClient::test_get_entity_state_api_error - TypeError: 'coroutine' object does not support the asynchronous context manager protocol
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHomeAssistantClient::test_get_entity_history_success - TypeError: 'coroutine' object does not support the asynchronous context manager protocol
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHomeAssistantClient::test_get_entity_history_default_end_time - TypeError: 'coroutine' object does not support the asynchronous context manager protocol
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHomeAssistantClient::test_get_entity_history_not_found - TypeError: 'coroutine' object does not support the asynchronous context manager protocol
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHomeAssistantClient::test_convert_ha_event_to_sensor_event - AssertionError: assert 'off' == 'of'
  - of
  + off
  ?   +
FAILED tests/unit/test_ingestion/test_ha_client.py::TestHomeAssistantClientWebSocketHandling::test_handle_event_processing - assert 0 == 1
 +  where 0 = len([])
FAILED tests/unit/test_adaptation/test_tracking_manager.py::TestSystemStatusAndMetrics::test_real_time_metrics_retrieval - assert None is not None
FAILED tests/unit/test_models/test_base_predictors.py::TestBasePredictor::test_prediction_history_management - AttributeError: type object 'ModelType' has no attribute 'GP'
FAILED tests/unit/test_models/test_base_predictors.py::TestLSTMPredictor::test_lstm_initialization - AssertionError: assert 'lstm_units' in {'alpha': 0.0001, 'dropout': 0.2, 'early_stopping': True, 'hidden_layers': [64, 32], ...}
 +  where {'alpha': 0.0001, 'dropout': 0.2, 'early_stopping': True, 'hidden_layers': [64, 32], ...} = LSTMPredictor(model_type=lstm, room_id=test_room, is_trained=False, version=v1.0).model_params
FAILED tests/unit/test_models/test_base_predictors.py::TestLSTMPredictor::test_lstm_training_convergence - src.core.exceptions.ModelTrainingError: Model training failed for lstm model in room 'test_room' | Error Code: MODEL_TRAINING_ERROR | Context: model_type=lstm, room_id=test_room | Caused by: ValueError: Found input variables with inconsistent numbers of samples: [150, 200]
FAILED tests/unit/test_models/test_base_predictors.py::TestLSTMPredictor::test_lstm_prediction_format - src.core.exceptions.ModelTrainingError: Model training failed for lstm model in room 'test_room' | Error Code: MODEL_TRAINING_ERROR | Context: model_type=lstm, room_id=test_room | Caused by: ValueError: Found input variables with inconsistent numbers of samples: [150, 10]
FAILED tests/unit/test_adaptation/test_tracking_manager.py::TestSystemStatusAndMetrics::test_active_alerts_retrieval - assert 0 == 2
 +  where 0 = len([])
FAILED tests/unit/test_models/test_base_predictors.py::TestLSTMPredictor::test_lstm_confidence_calibration - src.core.exceptions.ModelTrainingError: Model training failed for lstm model in room 'test_room' | Error Code: MODEL_TRAINING_ERROR | Context: model_type=lstm, room_id=test_room | Caused by: ValueError: Found input variables with inconsistent numbers of samples: [150, 200]
FAILED tests/unit/test_models/test_base_predictors.py::TestXGBoostPredictor::test_xgboost_initialization - AssertionError: assert 'objective' in {'colsample_bytree': 0.8, 'early_stopping_rounds': 20, 'eval_metric': 'rmse', 'learning_rate': 0.1, ...}
 +  where {'colsample_bytree': 0.8, 'early_stopping_rounds': 20, 'eval_metric': 'rmse', 'learning_rate': 0.1, ...} = XGBoostPredictor(model_type=xgboost, room_id=living_room, is_trained=False, version=v1.0).model_params
FAILED tests/unit/test_adaptation/test_tracking_manager.py::TestSystemStatusAndMetrics::test_alert_acknowledgment - assert False
FAILED tests/unit/test_models/test_base_predictors.py::TestXGBoostPredictor::test_xgboost_incremental_update - AttributeError: 'XGBoostPredictor' object has no attribute 'incremental_update'
FAILED tests/unit/test_models/test_base_predictors.py::TestHMMPredictor::test_hmm_initialization - AssertionError: assert 'n_iter' in {'covariance_type': 'full', 'init_params': 'kmeans', 'max_iter': 100, 'n_components': 4, ...}
 +  where {'covariance_type': 'full', 'init_params': 'kmeans', 'max_iter': 100, 'n_components': 4, ...} = HMMPredictor(model_type=hmm, room_id=bedroom, is_trained=False, version=v1.0).model_params
FAILED tests/unit/test_models/test_base_predictors.py::TestGaussianProcessPredictor::test_gp_initialization - AttributeError: type object 'ModelType' has no attribute 'GP'
FAILED tests/unit/test_models/test_base_predictors.py::TestPredictorSerialization::test_model_save_load_cycle - src.core.exceptions.ModelTrainingError: Model training failed for lstm model in room 'test_room' | Error Code: MODEL_TRAINING_ERROR | Context: model_type=lstm, room_id=test_room | Caused by: ValueError: Found input variables with inconsistent numbers of samples: [10, 200]
FAILED tests/unit/test_models/test_base_predictors.py::TestPredictorErrorHandling::test_invalid_feature_data - Failed: DID NOT RAISE <class 'src.core.exceptions.ModelPredictionError'>
FAILED tests/unit/test_models/test_ensemble.py::TestEnsembleTraining::test_ensemble_training_phases - src.core.exceptions.ModelTrainingError: Model training failed for ensemble model in room 'test_room' | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: ValueError: Length mismatch: Expected axis has 160 elements, new values have 640 elements
FAILED tests/unit/test_models/test_ensemble.py::TestEnsembleTraining::test_ensemble_model_weight_calculation - AssertionError: assert 'lstm' == 'xgboost'
  - xgboost
  + lstm
FAILED tests/unit/test_adaptation/test_tracking_manager.py::TestPerformanceAndConcurrency::test_concurrent_prediction_recording - AssertionError: assert 0 >= 10
 +  where 0 = <src.adaptation.tracking_manager.TrackingManager object at 0x7efd351d2660>._total_predictions_recorded
 +  and   10 = len([PredictionResult(predicted_time=datetime.datetime(2025, 8, 18, 11, 50, 59, 295554), transition_type='occupied', confidence_score=0.8, prediction_interval=None, alternatives=None, model_type=None, model_version=None, features_used=None, prediction_metadata={'room_id': 'room_0', 'prediction_id': 'pred_0'}), PredictionResult(predicted_time=datetime.datetime(2025, 8, 18, 11, 51, 59, 295575), transition_type='occupied', confidence_score=0.81, prediction_interval=None, alternatives=None, model_type=None, model_version=None, features_used=None, prediction_metadata={'room_id': 'room_1', 'prediction_id': 'pred_1'}), PredictionResult(predicted_time=datetime.datetime(2025, 8, 18, 11, 52, 59, 295582), transition_type='occupied', confidence_score=0.8200000000000001, prediction_interval=None, alternatives=None, model_type=None, model_version=None, features_used=None, prediction_metadata={'room_id': 'room_2', 'prediction_id': 'pred_2'}), PredictionResult(predicted_time=datetime.datetime(2025, 8, 18, 11, 53, 59, 295587), transition_type='occupied', confidence_score=0.8300000000000001, prediction_interval=None, alternatives=None, model_type=None, model_version=None, features_used=None, prediction_metadata={'room_id': 'room_0', 'prediction_id': 'pred_3'}), PredictionResult(predicted_time=datetime.datetime(2025, 8, 18, 11, 54, 59, 295592), transition_type='occupied', confidence_score=0.8400000000000001, prediction_interval=None, alternatives=None, model_type=None, model_version=None, features_used=None, prediction_metadata={'room_id': 'room_1', 'prediction_id': 'pred_4'}), PredictionResult(predicted_time=datetime.datetime(2025, 8, 18, 11, 55, 59, 295597), transition_type='occupied', confidence_score=0.8500000000000001, prediction_interval=None, alternatives=None, model_type=None, model_version=None, features_used=None, prediction_metadata={'room_id': 'room_2', 'prediction_id': 'pred_5'}), ...])
FAILED tests/unit/test_models/test_ensemble.py::TestEnsembleTraining::test_ensemble_training_error_handling - src.core.exceptions.ModelTrainingError: Model training failed for ensemble model in room 'test_room' | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: ValueError: could not convert string to float: 'in'
FAILED tests/unit/test_models/test_ensemble.py::TestEnsemblePrediction::test_ensemble_prediction_generation - src.core.exceptions.ModelPredictionError: Model prediction failed for ensemble model in room 'test_room' | Error Code: MODEL_PREDICTION_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: ValueError: Length mismatch: Expected axis has 10 elements, new values have 640 elements
FAILED tests/unit/test_adaptation/test_tracking_manager.py::TestPerformanceAndConcurrency::test_background_task_management - assert 0 > 0
FAILED tests/unit/test_models/test_ensemble.py::TestEnsemblePrediction::test_ensemble_confidence_with_gp_uncertainty - src.core.exceptions.ModelPredictionError: Model prediction failed for ensemble model in room 'test_room' | Error Code: MODEL_PREDICTION_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: ValueError: Length mismatch: Expected axis has 1 elements, new values have 640 elements
FAILED tests/unit/test_adaptation/test_validator.py::TestPredictionRecording::test_prediction_recording_with_metadata - TypeError: 'ValidationRecord' object is not subscriptable
FAILED tests/unit/test_adaptation/test_validator.py::TestPredictionRecording::test_duplicate_prediction_handling - TypeError: object of type 'ValidationRecord' has no len()
FAILED tests/unit/test_adaptation/test_validator.py::TestPredictionRecording::test_prediction_expiration_handling - AttributeError: 'PredictionValidator' object has no attribute '_cleanup_expired_predictions'
FAILED tests/unit/test_adaptation/test_validator.py::TestPredictionValidation::test_successful_prediction_validation - AttributeError: <src.adaptation.validator.PredictionValidator object at 0x7efd354ad8b0> does not have the attribute '_update_validation_in_db'
FAILED tests/unit/test_models/test_ensemble.py::TestEnsemblePrediction::test_ensemble_prediction_combination_methods - src.core.exceptions.ModelPredictionError: Model prediction failed for ensemble model in room 'test_room' | Error Code: MODEL_PREDICTION_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: ValueError: Length mismatch: Expected axis has 5 elements, new values have 640 elements
FAILED tests/unit/test_adaptation/test_validator.py::TestPredictionValidation::test_prediction_validation_multiple_candidates - AttributeError: <src.adaptation.validator.PredictionValidator object at 0x7efd354afdd0> does not have the attribute '_update_validation_in_db'
FAILED tests/unit/test_adaptation/test_validator.py::TestPredictionValidation::test_validation_with_no_pending_predictions - AttributeError: <src.adaptation.validator.PredictionValidator object at 0x7efd354ad220> does not have the attribute '_update_validation_in_db'
FAILED tests/unit/test_adaptation/test_validator.py::TestPredictionValidation::test_validation_time_window_enforcement - AttributeError: <src.adaptation.validator.PredictionValidator object at 0x7efd3521b8f0> does not have the attribute '_update_validation_in_db'
FAILED tests/unit/test_models/test_ensemble.py::TestEnsemblePrediction::test_ensemble_alternatives_generation - src.core.exceptions.ModelPredictionError: Model prediction failed for ensemble model in room 'test_room' | Error Code: MODEL_PREDICTION_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: ValueError: Length mismatch: Expected axis has 3 elements, new values have 640 elements
FAILED tests/unit/test_adaptation/test_validator.py::TestAccuracyMetricsRetrieval::test_time_filtered_accuracy_metrics - AttributeError: <src.adaptation.validator.PredictionValidator object at 0x7efd3521a300> does not have the attribute '_get_predictions_from_db'
FAILED tests/unit/test_adaptation/test_validator.py::TestValidationStatistics::test_validation_stats_collection - TypeError: object dict can't be used in 'await' expression
FAILED tests/unit/test_models/test_ensemble.py::TestEnsemblePrediction::test_ensemble_prediction_error_handling - src.core.exceptions.ModelPredictionError: Model prediction failed for ensemble model in room 'test_room' | Error Code: MODEL_PREDICTION_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: Exception: Model prediction failed
FAILED tests/unit/test_adaptation/test_validator.py::TestValidationStatistics::test_validation_performance_metrics - AttributeError: 'PredictionValidator' object has no attribute 'get_performance_stats'
FAILED tests/unit/test_adaptation/test_validator.py::TestValidationStatistics::test_total_predictions_counter - AttributeError: 'PredictionValidator' object has no attribute 'get_total_predictions'. Did you mean: '_total_predictions'?
FAILED tests/unit/test_adaptation/test_validator.py::TestValidationStatistics::test_validation_rate_calculation - AttributeError: 'PredictionValidator' object has no attribute 'get_validation_rate'. Did you mean: 'get_validation_stats'?
FAILED tests/unit/test_adaptation/test_validator.py::TestDatabaseIntegration::test_prediction_storage_to_database - TypeError: PredictionValidator._store_prediction_to_db() got an unexpected keyword argument 'room_id'
FAILED tests/unit/test_adaptation/test_validator.py::TestDatabaseIntegration::test_validation_update_in_database - AttributeError: 'PredictionValidator' object has no attribute '_update_validation_in_db'. Did you mean: '_update_predictions_in_db'?
FAILED tests/unit/test_adaptation/test_validator.py::TestDatabaseIntegration::test_predictions_retrieval_from_database - AttributeError: 'PredictionValidator' object has no attribute '_get_predictions_from_db'
FAILED tests/unit/test_adaptation/test_validator.py::TestCleanupAndMaintenance::test_expired_predictions_cleanup - TypeError: object of type 'ValidationRecord' has no len()
FAILED tests/unit/test_adaptation/test_validator.py::TestCleanupAndMaintenance::test_validation_history_cleanup - AttributeError: 'PredictionValidator' object has no attribute 'cleanup_old_predictions'. Did you mean: 'cleanup_old_records'?
FAILED tests/unit/test_adaptation/test_validator.py::TestCleanupAndMaintenance::test_pending_predictions_size_limit - TypeError: object of type 'ValidationRecord' has no len()
FAILED tests/unit/test_models/test_ensemble.py::TestEnsembleIncrementalUpdate::test_ensemble_incremental_update - src.core.exceptions.ModelTrainingError: Model training failed for ensemble model in room 'test_room' | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: ValueError: Shape of passed values is (640, 4), indices imply (640, 19)
FAILED tests/unit/test_adaptation/test_validator.py::TestErrorHandlingAndEdgeCases::test_validation_with_invalid_actual_time - assert False
 +  where False = isinstance(TypeError("PredictionValidator.validate_prediction() got an unexpected keyword argument 'actual_time'"), (<class 'ValueError'>, <class 'src.core.exceptions.OccupancyPredictionError'>))
FAILED tests/unit/test_adaptation/test_validator.py::TestErrorHandlingAndEdgeCases::test_concurrent_validation_operations - AttributeError: <src.adaptation.validator.PredictionValidator object at 0x7efd351c2cf0> does not have the attribute '_update_validation_in_db'
FAILED tests/unit/test_models/test_model_serialization.py::TestEnsembleModelSerialization::test_ensemble_base_model_serialization - AssertionError: assert 'placeholder' == 'test_room'
  - test_room
  + placeholder
FAILED tests/unit/test_models/test_model_serialization.py::TestSerializationErrorHandling::test_partial_model_data_loading - _pickle.PicklingError: Can't pickle <class 'unittest.mock.MagicMock'>: it's not the same object as unittest.mock.MagicMock
FAILED tests/unit/test_models/test_ensemble.py::TestEnsembleIncrementalUpdate::test_incremental_update_error_handling - Failed: Timeout (>30.0s) from pytest-timeout.
FAILED tests/unit/test_models/test_model_serialization.py::TestMultipleModelSerialization::test_model_comparison_after_serialization - src.core.exceptions.ModelPredictionError: Model prediction failed for xgboost model in room 'test_room' | Error Code: MODEL_PREDICTION_ERROR | Context: model_type=xgboost, room_id=test_room | Caused by: NotFittedError: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.
FAILED tests/unit/test_models/test_ensemble.py::TestEnsemblePerformance::test_ensemble_training_performance - src.core.exceptions.ModelTrainingError: Model training failed for ensemble model in room 'perf_test' | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=perf_test | Caused by: ValueError: Length mismatch: Expected axis has 100 elements, new values have 300 elements
FAILED tests/unit/test_models/test_model_serialization.py::TestBackwardsCompatibility::test_version_compatibility_handling - _pickle.PicklingError: Can't pickle <class 'unittest.mock.MagicMock'>: it's not the same object as unittest.mock.MagicMock
FAILED tests/unit/test_models/test_ensemble.py::TestEnsemblePerformance::test_ensemble_prediction_latency - src.core.exceptions.ModelPredictionError: Model prediction failed for ensemble model in room 'latency_test' | Error Code: MODEL_PREDICTION_ERROR | Context: model_type=ensemble, room_id=latency_test | Caused by: ValueError: Shape of passed values is (1, 4), indices imply (1, 19)
FAILED tests/unit/test_models/test_training_config.py::TestTrainingConfigManager::test_profile_management - AssertionError: Regex pattern did not match.
 Regex: 'Training profile invalid_profile not available'
 Input: "'invalid_profile' is not a valid TrainingProfile"
FAILED tests/unit/test_models/test_training_config.py::TestTrainingConfigManager::test_profile_updates - AssertionError: Regex pattern did not match.
 Regex: 'Profile invalid_profile not found'
 Input: "'invalid_profile' is not a valid TrainingProfile"
FAILED tests/unit/test_models/test_model_serialization.py::TestBasicModelSerialization::test_save_load_trained_xgboost_model - src.core.exceptions.ModelPredictionError: Model prediction failed for xgboost model in room 'test_room' | Error Code: MODEL_PREDICTION_ERROR | Context: model_type=xgboost, room_id=test_room | Caused by: NotFittedError: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.
FAILED tests/unit/test_models/test_training_pipeline.py::TestDataQualityValidation::test_data_quality_validation_good_data - assert True is True
 +  where True = DataQualityReport(passed=True, total_samples=721, valid_samples=721, sufficient_samples=True, data_freshness_ok=True, feature_completeness_ok=True, temporal_consistency_ok=True, missing_values_percent=0.0, duplicates_count=0, outliers_count=0, data_gaps=[], recommendations=[]).passed
FAILED tests/unit/test_models/test_training_pipeline.py::TestDataPreparationAndFeatures::test_feature_extraction - assert 0 > 0
 +  where 0 = len(Empty DataFrame\nColumns: []\nIndex: [])
FAILED tests/unit/test_models/test_training_pipeline.py::TestDataPreparationAndFeatures::test_data_splitting - assert 240 == 210
 +  where 240 = len(     temporal_hour  temporal_day_of_week  ...  contextual_temp  motion_count\n0                7                     1  ...        19.364462             1\n1               20                     1  ...        20.051141             1\n2               17                     6  ...        23.063995             3\n3                6                     6  ...        18.474568             3\n4                9                     0  ...        20.433625             4\n..             ...                   ...  ...              ...           ...\n235             18                     4  ...        22.007529             2\n236              1                     1  ...        29.530191             3\n237              3                     0  ...        13.853590             6\n238              3                     2  ...        25.114979             3\n239             21                     4  ...        22.867979             7\n\n[240 rows x 5 columns])
FAILED tests/unit/test_models/test_training_pipeline.py::TestModelTraining::test_model_training_failure_handling - AssertionError: Regex pattern did not match.
 Regex: 'No models were successfully trained'
 Input: "Model training failed for ensemble model in room 'test_room' | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: ModelTrainingError: Model training failed for ensemble model in room 'test_room' | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room"
FAILED tests/unit/test_models/test_training_pipeline.py::TestModelValidation::test_quality_threshold_checking - ValueError: could not convert string to float: 'in'
FAILED tests/unit/test_models/test_training_pipeline.py::TestModelDeployment::test_model_deployment - assert 0 == 1
 +  where 0 = len([])
FAILED tests/unit/test_models/test_training_pipeline.py::TestModelTraining::test_model_training_specific_type - src.core.exceptions.ModelTrainingError: Model training failed for ensemble model in room 'test_room' | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: ModelTrainingError: Model training failed for ensemble model in room 'test_room' | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room
FAILED tests/unit/test_models/test_training_pipeline.py::TestModelDeployment::test_model_artifact_saving - _pickle.PicklingError: Can't pickle <class 'unittest.mock.MagicMock'>: it's not the same object as unittest.mock.MagicMock
FAILED tests/unit/test_models/test_training_pipeline.py::TestModelValidation::test_model_validation_prediction_failure - ValueError: could not convert string to float: 'in'
FAILED tests/unit/test_models/test_training_pipeline.py::TestFullTrainingWorkflow::test_train_room_models_success - src.core.exceptions.ModelTrainingError: Model training failed for ensemble model in room 'test_room' | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: ValueError: could not convert string to float: 'in'
FAILED tests/unit/test_models/test_training_pipeline.py::TestFullTrainingWorkflow::test_train_room_models_quality_failure - ValueError: If using all scalar values, you must pass an index
FAILED tests/unit/test_models/test_training_pipeline.py::TestFullTrainingWorkflow::test_initial_training_multiple_rooms - AttributeError: <module 'src.models.training_pipeline' from '/home/runner/work/ha-ml-predictor/ha-ml-predictor/src/models/training_pipeline.py'> does not have the attribute 'get_config'
FAILED tests/unit/test_models/test_training_pipeline.py::TestFullTrainingWorkflow::test_train_room_models_insufficient_data - AssertionError: Regex pattern did not match.
 Regex: 'Insufficient data'
 Input: "Model training failed for ensemble model in room 'test_room' | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
FAILED tests/unit/test_models/test_training_pipeline.py::TestTrainingPipelineErrorHandling::test_pipeline_exception_handling - AssertionError: Regex pattern did not match.
 Regex: 'Training pipeline failed'
 Input: "Model training failed for ensemble model in room 'test_room' | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: Exception: Database connection failed"
FAILED tests/unit/test_models/test_training_pipeline.py::TestTrainingPipelineErrorHandling::test_retraining_pipeline_error_handling - AssertionError: Regex pattern did not match.
 Regex: 'Retraining pipeline failed'
 Input: "Model training failed for ensemble model in room 'test_room' | Error Code: MODEL_TRAINING_ERROR | Context: model_type=ensemble, room_id=test_room | Caused by: Exception: Retraining failed"
ERROR tests/unit/test_features/test_contextual.py::TestContextualFeatureExtractorEdgeCases::test_no_room_states
ERROR tests/unit/test_adaptation/test_validator.py::TestAccuracyMetricsRetrieval::test_room_accuracy_metrics - TypeError: 'predicted_time' is an invalid keyword argument for Prediction
ERROR tests/unit/test_adaptation/test_validator.py::TestAccuracyMetricsRetrieval::test_overall_accuracy_metrics - TypeError: 'predicted_time' is an invalid keyword argument for Prediction
ERROR tests/unit/test_adaptation/test_validator.py::TestAccuracyMetricsRetrieval::test_model_specific_accuracy_metrics - TypeError: 'predicted_time' is an invalid keyword argument for Prediction
ERROR tests/unit/test_adaptation/test_validator.py::TestAccuracyMetricsRetrieval::test_accuracy_trend_analysis - TypeError: 'predicted_time' is an invalid keyword argument for Prediction
ERROR tests/unit/test_adaptation/test_validator.py::TestValidationStatistics::test_room_prediction_counts - TypeError: 'predicted_time' is an invalid keyword argument for Prediction
===== 154 failed, 599 passed, 56157 warnings, 6 errors in 94.38s (0:01:34) =====